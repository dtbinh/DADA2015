
\subsubsection{WP2 Proxemics}



In this workpackage we are interested in modeling behaviors of group of agents while conversing and while moving around. We will pay particular attention at the social interaction of the agents during these activities. We will also develop an animation model that incorporate two models: statistical model as developed in WP1 and procedural model developed within the Greta plateform.


\paragraph{Task 2.1: Group behaviors during multi-way conversation}


In this task we will model multi-party conversation behaviors. We will focus on turn-taking management. While indication of what the agents would say to whom and when will be provided by a script (Task 3.1 and Task 4.X), the turn-taking model will instantiate which behaviors the agents will display. Gaze, body orientation, position in space are important cues for indicating who has the turn, who wants to keep it, to give it to someone, who listens? We will extend an existing turn-taking model (Ravenet et al., 2014) that is based on Sack?s model (Sack et al, 1974), that embeds F-Formation (Kendon, 1990) and that takes into account social attitude of the agents toward each other. This model is implemented as a state machine where the states are defined by the turn-taking and correspond to conversational roles. Transition between states is triggered when an agent changes conversational role. Attitudes vary the behavior of the agents such as their propensity to gaze at others. We will extend this model to simulate different configurations of speech overlap such as terminal overlaps, conditional access to the turn, and choral (Schegloff, 2000) as well as long silences when nobody takes the turn. We will add further states to encompass more conversational functions (eg greeting, word search?). We will also model that transitions from one state to another one can bring the agents of a group to be in the same state (parallel configuration as when greeting each other or laughing together).

A version of this model will be instantiated to model tri-partite interaction between two virtual actors and the audience (viewed as a virtual actor taking part of the interaction).


\paragraph{Task 2.2 : Group behaviors during stage movements}


This involves implementation of advanced « steering behaviors » such as follow, flee, separate, join, merge, enter stage, exit stage, etc.


This task will model agents? behavior when moving around in the environment. The animation of the virtual agent doing some tasks will be given by WP1. It will not focus on path planning as this information will be provided by a script (Task 3.1 and Task 4.X). Rather it will model how agents perform displacement in social settings. Gaze direction, body orientation and spatial distance to other agents will be computing for different ?steering behaviors?. These features will be modeled through different synchronization mechanisms: moving in synch, moving ahead, following, etc. They evolve dynamically in function of each agent?s position and orientation in space. The basic animation of the agent, ie without any influence from surrounding agent, is given by WP1. To simulate the dynamic evolution of agents' behaviors we will make use of Neural Network simulation (Prepin 2013) where we can render how behaviors of one actor can act on behaviors of other actors (eg walking powerfully toward an actor with an angry expression will result in moving backward of another actor with a less dominant attitude. Mutual coupling of behaviors will be modeled as emerging from such action-reactive behavior simulation (Prepin 2013) ensuring not only the synchronization between actors? behaviors but also their mutual influence.

As for Task 2.2, a version of this model will be instantiated to consider the audience as one virtual actor.



\paragraph{Task 2.3:  Combination of statistical and procedural models.}


In this task we will develop an animation model that will merge animations coming from statistical model developed in WP1 and procedural model developed in WP2 (Task 2.1 and Task 2.2). This blend is required for the interaction settings where behaviors of the agents are driven by both animation models.

The procedural model relies on forward and inverse kinematic models (Huang, 2012). It controls the arms position, gaze direction and body orientation. The statistical model (from WP1) controls the whole body. Our animation blender model will work at the modalities level and will also incorporate movement propagation; that is how motion of one body part affects other body parts. At first, the animation blender model will merge whole body motion computed by the statistical model as specific body motion computed by the procedural model. More precisely, arms position, gaze direction and body orientation outputted by the procedural model will be viewed as constraints to be reached. These motions will be added onto the animation computed by statistical model; the position of the arms, head and torso computed by the procedural model will overwrite those computed by the statistical model. In a second step, the animation blender model will incorporate propagation of movements. To compute movement propagation we will develop a statistical model that learns which motion is due to action and which motion is due to movement propagation.




\endinput
