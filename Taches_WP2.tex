
\subsubsection{WP2 Proxemics}


\begin{center}
\begin{tabular}{|l|l|}\hline
WP2 &  Proxemic component \\\hline
Responsable &  LTCI  \\\hline
Participants &  Inria, LIF\\\hline
Duration  &   42 months \\\hline
Objectives &  Design and development of low-dimensional, multi-character  animation \\\hline
Content &  \\\hline
Task 21 & Communicative behaviours   ($T_0 \rightarrow  T_0+24$)\\\hline
Task22 &  Steering behaviours  ($T_0+12 \rightarrow  T_0+36$) \\\hline
Task23 &  Combination of statistical and procedural models    ($T_0 +24 \rightarrow  T_0+36$) \\\hline
\end{tabular}
\end{center}

In this workpackage we are interested in modeling behaviors of group of agents while conversing and while moving around. We will pay particular attention at the social interaction of the agents during these activities. We will also develop an animation model that incorporates two models: statistical model as developed in WP1 and procedural model developed within the Greta platform.


\paragraph{Task 2.1: Group behaviors during multi-way conversation}


In this task we will model multi-party conversation behaviors. We will focus on turn-taking management. While indication of what the agents would say to whom and when will be provided by a script (Task 3.1 and Task 4.X), the turn-taking model will instantiate which behaviors the agents will display. Gaze, body orientation, position in space are important cues for indicating who has the turn, who wants to keep it, to give it to someone, who is listening, etc.  We will extend an existing turn-taking model \cite{ RavenetCOP14} that is based on Sack's model \cite{SAC74}, that embeds F-Formation \cite{Kendon90}  and that takes into account social attitude of the agents toward each other. This model is implemented as a state machine where the states are defined by the turn-taking and correspond to conversational roles. Transition between states is triggered when an agent changes conversational role. Attitudes vary the behavior of the agents such as their propensity to gaze at others. We will extend this model to 
simulate different configurations of speech overlap such as terminal overlaps, conditional access to the turn, and choral \cite{Schegloff2000} as well as long silences when nobody takes the turn. We will add further states to encompass more conversational functions (eg greeting, word search). We will also model that transitions from one state to another one can bring the agents of a group to be in the same state (parallel configuration as when greeting each other or laughing together).


\paragraph{Task 2.2 : Group behaviors during stage movements}

This task will model agent behaviors when moving around in the environment,  included such  advanced  steering behaviors as follow, flee, separate, join, merge, enter stage, exit stage, etc. The animation of the virtual agent doing some tasks will be given by WP1. It will not focus on path planning as this information will be provided by a script (Task 3.1 and Task 4.X). Rather it will model how agents perform displacement in social settings. Gaze direction, body orientation and spatial distance to other agents will be computing for different {\em steering behaviors}. These features will be modeled through different synchronization mechanisms: moving in synch, moving ahead, following, etc. They evolve dynamically in function of each other's  positions and orientations in space. The basic animation of the agent, ie without any influence from surrounding agent, is given by WP1. To simulate the dynamic evolution of agent behaviors,  we will make use of Neural Network simulation \cite{Prepin2013} where we can 
render how behaviors of one actor can act on behaviors of other actors (eg walking powerfully toward an actor with an angry expression will result in moving backward of another actor with a less dominant attitude. Mutual coupling of behaviors will be modeled as emerging from such action-reactive behavior simulation \cite{Prepin2013} ensuring not only the synchronization between actor behaviors but also their mutual influence. 


\paragraph{Task 2.3:  Combination of statistical and procedural models.}

In this task we will develop an animation model that will merge animations coming from statistical model developed in WP1 and procedural model developed in WP2 (Task 2.1 and Task 2.2). This blend is required for the interaction settings where behaviors of the agents are driven by both animation models.  The procedural model relies on forward and inverse kinematic models \cite{huang:2012:EET}. It controls the arms position, gaze direction and body orientation. The statistical model (from WP1) controls the whole body.  Our animation blender model will work at the modalities level and will also incorporate movement propagation; that is how motion of one body part affects other body parts. At first, the animation blender model will merge whole body motion computed by the statistical model as specific body motion computed by the procedural model. More precisely, arms position, gaze direction and body orientation outputted by the procedural model will be viewed as constraints to be reached. These motions will be 
added onto the animation computed by statistical model; the position of the arms, head and torso computed by the procedural model will overwrite those computed by the statistical model. In a second step, the animation blender model will incorporate propagation of movements. To compute movement propagation we will develop a statistical model that learns which motion is due to action and which motion is due to movement propagation.

\vspace{5mm}

\begin{tabular}{|l|p{10cm}|l|}\hline
Deliverables & Name and content  & Date  \\\hline
L2.1  & Report on the state of the art of proxemics models in computer animation&   $T_0+12$  \\\hline
L2.2  &  First version of the models : Prototype (software) and its documentation (Report on the models developed) & $T_0+24$ \\\hline
L2.3  &  Second version of the models : Prototype (software) and its documentation (Report on the models developed) &  $T_0+36$ \\\hline
\end{tabular}

\paragraph{Partner's roles} This work package  will be coordinated by LITC. Inria  will contribute to tasks 2.2 and 2.3. LIF will contribute to task 3.3.
 
\endinput
