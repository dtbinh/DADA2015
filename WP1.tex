% Recent work has demonstrated such models for the case of locomotion believable controllers,
% gesture controllers (Levine 2010) and face controllers (Ding et al., 2013). We will aim to generalize this previous work
% to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting
% objects, in a variety of expressions and moods.

thierry : ce passage là doit probablement etre remonté au dessus pour l'explication générale du flux entre WPs. This workpackage focuses on animation models for isolated actors. 
The inputs that are used by the methods to be developped in this WP are procedural animation scenario as ouput by WP2. 
Such a scenario includes in particular detailed indications on the action to be realized (walk from one point to another, carry object, knock on door, throw object, lift object, move object), the mood of the character (neutral, happy, afraid, angry, anxious, sad, proud, shameful) and a set of static information about the character that change the way people move (age, gender, morphology, corpulence, expressivity level, etc). Both action and mood may vary with time almong the animation while static information remain fixed per nature. 
These three sets of information will be refered hereafter as \textit{action context}, \textit{mood context} and \textit{profile context}. . 


\subsubsection{WP1. Kinesic component} 

This WP aims at creating multi-modal statistical models of individual body movements from annotated, mainly from mocap data, to generate novel expressive animation suitable for dramatic performances. To do so we will tackle few difficult and open problems: Learning full body animation models for many settings rang-
ing from emotional state to actor profile (corpulence, expressivity level etc). 
Moreover while the animation model will be learned from a limited number of actors’ data we want it to be able to be remapped to other actors. 
Next we will have to combine multiple animation models in order to take advantage of all exiisting works, in particular for the animation of the face. 
Finally we will investigate learning animation models for new gestures and activities from only few training samples which will allow enriching the system easily by avoiding the costly and tedious task of gathering a large corpus of training data as
usually required in statistical machine learning.

% Thierry : parler plutot de 3 verrous scientifiques. We will organize the work in three tasks, the two first tasks are dedicated to modeling and synthesizing animations. We will distinguish between animating the whole body and animating the face which will be animated based on a given scenario but also based on the scenario from other actors to enable accurate interaction between actors (e.g. gazes). Why ? The last task is focused on making the models designed in thee two first tasks able to learn from few training data


A first scientific lock is how to design generic body controllers able to synthesize the animation of the full body of a character. 
Designing a generic model that fit many settings is an elegant way for producing smooth animation of complex motions (e.g. a character walks nervouslay towards a chair, then he sits down and becomes quiet, then he takes something on the ,table), which otherwise require artifical smoothing and postprocessing. We will first explore extending prevous works on contextual markovian models \cite{Radenen2014, Ding2013, Ding2014} which are a variant of Hidden Markov Models (HMMs) that have shown strong potential for designing face controllers. These models will serve as a baseline. 
Next, we will investigate the use of continuous state space models and (deep) recurrent neural networks. Such models have shown strong abilities for dealing with complex signals like speech and handwriting \cite{Graves, Schmidhuber}. 
The main difficulty here will lie here in imagining ways to mixthe idea of contextal variales in these models in order to integrate contextual information that would modify the behaviour of the models. 
We plan want to transform these models as we did for markovian models for aking into account contextual information. 
Finally we plan to explore alternative strategies such as using neuro muscular based models following ideas like the one of deltalognormal models from \cite{O'Reilly, 2012} which allow recovering the sequence of neuromuscular commands that generated a handwritten gesture. 

% We will also investigate new ways for parameterizing the animation model with few variables related to the emotional state of an actor and more generally to an actor’s profile. The difficulty here lies in the definition of defining the representation of an actor, we will attempt to learn it directly from data using recent techniques named as representation learning (Contardo 2014). 

A second lock will concern the animation of the face in dialogue situations. Since face animation is particularly difficult, and given that we have worked previously with three complementary methods, we will focus here on how to mix few face animation coming from different tools: mocap based animation, video-based animation, and procedural anmation. 
The latter animation model is already working and part of the GRETA system. The mocap based animation model will be easily built on previous works by the team \cite{YuThesis}. 
Finally starting from previous work on visual prosody we will design the third model ... Ideally, this should be done without MOCAP data, using only audio and video processing, possibly enhanced with depth (kinect). To be continued (Rémi)...We will explore strategies for optimally combining these three animation models...

Finally while the animation models will be learned from a limited number of actors' data we want it to be able to be remapped to other actors. 
In addition we want our animation models easily extendabke to new activities and moods, by making them learnable from only few training samples. 
This  will allow enriching the system easily whitout a costly and tedious task of gathering a large corpus of training data as usually required in statistical machine learning. Learning models, and particularly statistical models from few samples is a key and open issue \cite{One shot, Zeroshot}. 
To enable learning from few samples we will mainly explore two ways that aim at favoring transfer from learning one
gesture model to learning another gesture model. 
First, few preliminary works have shown that statistical markovian models as in (Ding et al., 2013) could be defined in such a way that the data from all gesture classes could be exploited to learn models for all gesture classes. 
Second using continuous state space models with a low dimensional state space
(corresponding to the degree of freedom of body poses) should permit characterizing a particular gesture as its dynamic
in this latent space whose limited dimension would enable learning from few samples.


% We will mainly investigate two approches for extending approaches developped in task T11 to enable learning from few samples. The first strategy consists in extending the idea of context variables that models of task T11 rely on in order to design a global model for all actions.
% In the case of markovian model for instance this means that instead of definig one model per action one could define a unique big markovian model where every state would stand for a particular position of the body and performing an action would correspond to following a path (i.e. a state sequence) in this big model. The model for a particular action would correspond then in a bundle of paths in this model.  Doing so one could expect that all the training data (whatever the action it corresponds to) could be exploited to learn all the states of this big markovian model, hence implementing some kind of transfer learning between actions. A new action would correspond then in a bundle of paths in this model and could be learnt from few samples only. Preliminary works that we did let us expect that such a strategy would work with statistical markovian models (Ding et al., 2013). In this case the above idea is implementing by introducing new contextual variables, which are indicators of the action to perform, 
% that modify the gaussian densities. We will first investigate this strategy deeper for contextual markovien models then we will extend this approach to neural networks. 
% 
% Second we will explore the use of using continuous state space models with a low dimensional state space (e.g. corresponding to the degree of freedom of body poses or to the neuro muscular commands) which should permit characterizing a particular motion or gesture as its dynamic in this latent space whose limited dimension would enable learning from few samples.  

All along the project we will rely as much as possible on existing datasets. For instance Mocap data of considered actions have already been recorded by C. Pelachaud within the project Feder Anipev (http://www.anipev.com/). 
The corpus EMILYA (EMotional body expressIon in daiLY Actions databaseBodily Emotional Actions Behavior) (Fourati, 2014) is constituted of 7 actions performed by 11 actors with 8 emotions. The actions encompass everyday actions such as walking, carrying an object, and sitting. 
The emotions cover the positive and negative spectrum. 


\endinput