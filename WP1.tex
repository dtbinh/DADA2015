
\subsubsection{WP1. Kinesic component} 

Procedural animation models for isolated actors. We will create multi-modal statistical models of individual body movements from annotated mocap data to generate novel expressive animation suitable for  dramatic performances. Recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers (Levine 2010) and face controllers (Ding et al., 2013). We will aim to generalize this previous work to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting objects, in a variety of expressions and moods. 

To do so we will tackle few difficult and open problems: Learning full body animation models for many settings rang-ing from emotional state to actor profile (corpulence, expressivity level etc). Moreover while the animation model will be learned from a limited number of actors' data we want it to be able to be remapped to other actors. In addition we will investigate learning animation models for new gestures and activities from only few training samples which will allow enriching the system easily by avoiding the costly and tedious task of gathering a large corpus of training data as usually required in statistical machine learning.

To achieve these goals we will focus on the design of generic animation models rather than of a large number of models. We want to design and learn a generic model that could be instantiated in various settings. We will first explore extensions of previous promising works (e.g. Radenen 2014). Then we will explore and propose new ways for parame-terizing the animation model with few variables related to the emotional state of an actor and more generally to an ac-tor's profile. The difficulty here lies in the definition of defining the representation of an actor, we will attempt to learn it directly from data using recent techniques named as representation learning (Contardo 2014). Second we will focus on enhancing the modeling formalism for capturing and modeling the smooth transition between successive actions or gestures. We will investigate here two lines of research, the first one would be the extension of deltalognormal models from (O'Reilly, 2012) which will allow working on the neuromuscular commands 
that generate gestures rather than on gestures, the second line consists of using continuous state space models which would allow modeling and synthesizing smoother transitions between successive actions.

To enable learning from few samples we will mainly explore two ways that aim at favoring transfer from learning one gesture model to learning another gesture model. First, few preliminary works have shown that statistical markovian models as in (Ding et al., 2013) could be defined in such a way that the data from all gesture classes could be exploited to learn models for all gesture classes. Second using continuous state space models with a low dimensional state space (corresponding to the degree of freedom of body poses) should permit characterizing a particular gesture as its dynamic in this latent space whose limited dimension would enable learning from few samples.  

We will rely as much as possible on existing datasets. For instance Mocap data of considered actions have already been recorded by C. Pelachaud within the project Feder Anipev (http://www.anipev.com/). The corpus EMILYA (EMotional body expressIon in daiLY Actions databaseBodily Emotional Actions Behavior) (Fourati, 2014) is constituted of 7 actions performed by 11 actors with 8 emotions. The actions encompass everyday actions such as walking, carrying an object, and sitting. The emotions cover the positive and negative spectrum. 
This task will be led by LIF (University of Marseille), with contributions from Inria and Telecom ParisTech.

\endinput