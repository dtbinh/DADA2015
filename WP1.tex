
\subsubsection{WP1. Kinesic component} 

This workpackage focuses on animation models for isolated actors. The inputs that are used by the methods to be developped in this WP are procedural animation scenario as ouput by WP2. Such a scenario includes in particular detailed indications on the action to be realized (walk from one point to another, carry object, knock on door, throw object, lift object, move object), the mood of the character (neutral, happy, afraid, angry, anxious, sad, proud, shameful) and a set of static information about the character that change the way people move (age, gender, morphology, corpulence, expressivity level, etc). Both action and mood may vary with time almong the animation while static information remain fixed per nature. These three sets of information will be refered hereafter as \textit{action context}, \textit{mood context} and \textit{profile context}. . 

TA : il faut decrire un peu plus pécisément à quoi peut ressembler l'input de ce WP qui vient de WP2\\

This WP aims at creating multi-modal statistical models of individual body movements from annotated mocap data to generate novel expressive animation suitable for  dramatic performances. We will focus on the design of generic animation models rather than of a large number of models. We want to design and learn a generic model that could be instantiated in various settings. Moreover while the animation model will be learned from a limited number of actors' data we want it to be able to be remapped to other actors. In addition we will investigate learning animation models for new gestures and activities from only few training samples which will allow enriching the system easily by avoiding the costly and tedious task of gathering a large corpus of training data as usually required in statistical machine learning.  


We will rely as much as possible on existing datasets. For instance Mocap data of considered actions have already been recorded by C. Pelachaud within the project Feder Anipev (http://www.anipev.com/). The corpus EMILYA (EMotional body expressIon in daiLY Actions databaseBodily Emotional Actions Behavior) (Fourati, 2014) is constituted of 7 actions performed by 11 actors with 8 emotions. The actions encompass everyday actions such as walking, carrying an object, and sitting. The emotions cover the positive and negative spectrum. 

\paragraph{Generic full body animation model}

We will first focus on the design of generic body controllers able to synthesize the animation of the full body of a character (through the sequence of mocap representation) for a given procedural animation scenario as output by WP2, i.e. a sequence of actions realized for a particular mood context and for a specific character profile. We will first consider that there is one model per action and that the animation produced by such a model should take into account, as inputs, the mood context as well as the character profile context. We will consider modeling frameworks that allow separating the mood components from the profile components so as to enable extrapolating moods to other profiles, and profiles to other moods. Whatever the models under investigation we will pay attention to focus on strategies that enable synthesizing smooth transition between successive actions, moods, or gestures. 

We will first explore extensions of previous promising works (e.g. Radenen 2014). Then we will explore and propose new ways for parameterizing the animation model with few variables related to the emotional state of an actor and more generally to an actor's profile. We plan to investigate the following lines of research:
 
\begin{itemize}  
  \item Firstly we will investigate contextual markovian models where gaussian probability density functions associated to states are parameterized by contextual observation (mood and profile information here). Recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers (Levine 2010) and face controllers \cite{Ding2013, Ding2014}. We will aim to generalize this previous work to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting objects, in a variety of expressions and moods. These models will serve as a baseline for evaluating the new modeling approaches we describe bellow. 
  \item Second, we will investigate the use of deep neural networks and of dynamic versions of these (i.e. recurrent neural nets) which have demonstrated strong abilities to model, to classify and to synthesize complex signals such as speech or handwriting \cite{HintonDeepSpeech, Deng_Speech_Reco, GravesHWR_Reco, GravesHWR_Synth}. These models are related to what is called representation learning which emerged in the last few years as a key topic in the machine learning community \footnote{See the recently born ICLR conference on Learning Representations at http://www.iclr.cc/} (Contardo 2014). One main difficulty will be to integrate the use of contextual information as input in order to modify the behaviour of the models. We plan to extend the principle of contextual markovian models to neural nets by investigating ideas like designing bilinear layers in the neural net where weights could be defined as a function of the contextual input, inspired by works like \cite{Bilinear, Multiview}.
  \item At last we will investigate neuro muscular based models following ideas like \cite{O'Reilly et al., 2012} which aims at recovering from a handwritten signal the sequence of neuromuscular commands that generated the handwriting signal. The underlying idea here is to exploit such models in order to work in a new representation space, the space of neuromuscular commands that generate motion, rather than on the observed motion itself. Although such models have not been used to model complex gestures up to now it is expected that they could be robust enough to provide good estimation of the command sequence. The main advantage of such a change of representation space is an expected reduction of the dimension of this space, enabling easier learning from few samples and transfer learning (as will be investigated in task 13). This would be linked to (Gaussian process reference).

\end{itemize}

\paragraph{Interaction animation} 

This subtask focuses on learning models of gesture and facial expressions in dialogue situations.Based on previous work on  visual prosody, we would like to learn joint models of gesture and  speech prosody. Ideally, this should be done without MOCAP data, using only audio and video processing, possibly enhanced with depth (kinect). To be continued (Rémi)...



\paragraph{Learning from few samples} 


% To enable learning from few samples wewill mainly explore two ways that aim at favoring transfer from learning one gesture model to learning another gesture model. First, few preliminary works have shown that statistical markovian models as in (Ding et al., 2013) could be defined in such a way that the data from all gesture classes could be exploited to learn models for all gesture classes. 
% 

We will mainly investigate two approches for extending approaches developped in task T11 to enable learning from few samples. The first strategy consists in extending the idea of context variables that models of task T11 rely on in order to design a global model for all actions.
In the case of markovian model for instance this means that instead of definig one model per action one could define a unique big markovian model where every state would stand for a particular position of the body and performing an action would correspond to following a path (i.e. a state sequence) in this big model. The model for a particular action would correspond then in a bundle of paths in this model.  Doing so one could expect that all the training data (whatever the action it corresponds to) could be exploited to learn all the states of this big markovian model, hence implementing some kind of transfer learning between actions. A new action would correspond then in a bundle of paths in this model and could be learnt from few samples only. Preliminary works that we did let us expect that such a strategy would work with statistical markovian models (Ding et al., 2013). In this case the above idea is implementing by introducing new contextual variables, which are indicators of the action to perform, that modify the gaussian densities. We will first investigate this strategy deeper for contextual markovien models then we will extend this approach to neural networks. 

Second we will explore the use of using continuous state space models with a low dimensional state space (e.g. corresponding to the degree of freedom of body poses or to the neuro muscular commands) which should permit characterizing a particular motion or gesture as its dynamic in this latent space whose limited dimension would enable learning from few samples.  


\endinput