
% 
% \subsubsection{WP1. Related works}
% 
% Animation of an avatar is usually tackled by working separately on the full body animation model on the one hand and on the face (and gesture) animation model on the other hand (since the latter animation strongly depends on the dialogue the avatar is engaged in), where the animation produced by the two models are merged to produce a final complete animation \cite{DBLP:journals/tvcg/ShoulsonMKB14}. 
% 
% Full body kinematic animation (or control) consists in animating the full body of an avatar while he is performing actions such as walking, dancing, sitting etc. Although there has been lots of work on this subject it is still a challenging problem due to the high dimensionality of the character's configuration. Data-driven approaches are very popular here and make of use motion-capture data to learn animation models which, once learned, may be used to animate a virtual character to perform a given task. 
% Many systems have been proposed for producing animation models and controlers, they usually are based on statistical models such as Hidden Markov Models (HMMs) \cite{DBLP:journals/tog/LevineTK09} and Conditional Random Fields (CRFs) \cite{DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14}. Most accurate methods exploit a large dataset of motions where one can synthesize a complete motion sequence corresponding to a particular task by using warping or blending strategies of motions in the training set \cite{WitkinAndPopovic1995}. Locomotion controllers have been proposed that concatenate motion
% clips from a motion capture dataset to produce an animation that is smooth [Treuille et al. 2007; Mc-Cann and Pollard 2007]. High-quality kinematic controllers have been built from this idea by using a {\it motion graph}, which is a graph structure that describe how clips from a dataset can be reordered into new motions \cite{LeeEtAl2002}. 
% While locomotion controllers are driven
% by direct high-level commands (such as desired movement direction), no such clear control signal is available for body language. To animate the face, and accompanying arm gestures, many works have focused on developing specific animation models based on a dialogue related input, either speech, text or prosody features \cite{DBLP:journals/tog/LevineTK09, DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14, NOUS}. 
% At the end, recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers (Levine 2010) and face controllers (Ding et al., 2013). 
% % We will aim to generalize this previous work
% % to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting
% % objects, in a variety of expressions and moods.
% Yet all these statistical approaches require large annotated datasets to work well.
% 
% Thereby these approaches do not easily work with small training sets which is a key issue, as stressed for instance in \cite{DBLP:journals/tog/LevineWHPK12}, since first it requires considerable effort and time to build large datasets, and second because many applications demand unique motion styles and require their own datasets. This has led a number of researchers to put the effort on designing models that may be easily learned from a few samples. One main approach for doing so lies in the use (or learning) of a continuous state space to represent the data, making learning in this low dimensional space much easier  \cite{DBLP:journals/tog/LevineWHPK12, DBLP:conf/atal/ChiuM14}. A relevant technology for this are Gaussian Process which have been extended for dealing wiuth dynamic data in \cite{DBLP:journals/pami/WangFH08}.
% 
% These latter models are not far from recurent neural networks, and to Long Short Term Memory neural networks in particular \cite{DBLP:conf/nips/HochreiterS96, DBLP:journals/corr/GreffSKSS15}, that have been shown recently to work well for complex signals such as speech and handwriting, for recognition tasks \cite{DBLP:conf/nips/GravesS08} as well as for synthesis tasks \cite{DBLP:journals/corr/Graves13}. These models are part of a current trend in machine  learning called representation learning (see the recently born conference ICLR at http://www.iclr.cc/) which aims at discovering relevant and usually low dimensional representation of the data under investigation (the pionner work of this domaine is the one by G. Hinton in Science \cite{DBLP:journals/cogsci/HintonOWT06}.
% 
% 
% % High-quality kinematic con-
% % trollers have been created for tasks such as boxing [Lee and Lee
% % 2006] and locomotion [McCann and Pollard 2007; Treuille et al.
% % 2007; Lo and Zwicker 2008; Lee et al. 2009], using a discrete
% % representation of motion data called a motion graph, which uses
% % a graph structure to describe how clips from an example library can
% % be reordered into new motions [Arikan and Forsyth 2002; Kovar
% % et al. 2002; Lee et al. 2002]. 
% 
% 
% 
% % 
% % ADAPT : Interactive control of virtual characters can be
% % achieved by searching through motion clip samples for desired mo-
% % tion as an unsupervised process [Lee et al. 2002], or by extracting
% % descriptive parameters from motion data [Johansen 2009]. Proce-
% % dural methods are used to solve specific tasks such as reaching, and
% % can leverage empirical data [Liu and B 2003], example mo-
% % tions [Feng et al. 2012b], or hierarchical inverse kinematics [Baer-
% % locher and Boulic 2004] for more natural movement. Physically-
% % based approaches [Faloutsos 2002; Yin et al. 2007] derive con-
% % trollers to simulate character movement in a dynamic environment.
% % We refer to Pettr ÃÅ et. al. [2008] for a more extensive summary of
% % work in these areas.
% 
% 
% % 
% % Real time prosody : \cite{DBLP:journals/tog/LevineTK09}
% % We present a data-driven method that automatically generates
% % body language animation from the prosody of the participant‚Äôs
% % speech signal. The system is trained on motion capture data of real
% % people in conversation, with simultaneously recorded audio. 
% % 
% % To generate the animation, we select appropriate gesture subunits
% % from the motion capture training data based on prosody cues in the
% % speech signal. 
% 
% 
% 
% % Gesture Controlers : \cite{DBLP:journals/tog/LevineKTK10}
% % Prior speech-based gesture synthesis techniques directly associate
% % animation segments with prosody features [Levine et al. 2009].
% % Such methods are sensitive to the amount and quality of training
% % data, and suffer heavily from extraneous, accidental associations,
% % known as overfitting. I
% % 
% % The development of gesture controllers was inspired by the recent
% % introduction of online locomotion controllers that assemble motion
% % clips from motion capture to produce an animation that is smooth,
% % natural, and satisfies a set of constraints [Treuille et al. 2007; Mc-
% % Cann and Pollard 2007]. While locomotion controllers are driven
% % by direct high-level commands (such as desired movement direc-
% % tion), no such clear control signal is available for body language.
% % 
% 
% % 
% % Learning few samples
% % 
% % GP pour humain motion \cite{DBLP:journals/pami/WangFH08}
% 
% % Levine :Continuous character control .. \cite{DBLP:journals/tog/LevineWHPK12} 
% % Central to a method‚Äôs ease of use is its capacity to synthesize character motion for novel
% % situations without requiring excessive data or programming effort.
% % 
% % Full body kinematic control is a challenging problem, due to the
% % high dimensionality of the character‚Äôs configuration and the diffi-
% % culty of avoiding low quality motions. High-quality kinematic con-
% % trollers have been created for tasks such as boxing [Lee and Lee
% % 2006] and locomotion [McCann and Pollard 2007; Treuille et al.
% % 2007; Lo and Zwicker 2008; Lee et al. 2009], using a discrete
% % representation of motion data called a motion graph, which uses
% % a graph structure to describe how clips from an example library can
% % be reordered into new motions [Arikan and Forsyth 2002; Kovar
% % et al. 2002; Lee et al. 2002]. However, while graphs are well suited
% % for representing large datasets, smaller datasets that lack extensive
% % transitions and variations may be inadequate to induce an expres-
% % sive discrete graph. 
% % 
% % Marsella : Gesture generation ... \cite{DBLP:conf/atal/ChiuM14}
% % 
% % 
% % Recurrent NNs:
% % Graves offline \cite{DBLP:conf/nips/GravesS08}
% % synythese handwriting \cite{DBLP:journals/corr/Graves13}
% % LSTM old \cite{DBLP:conf/nips/HochreiterS96}
% % LSTM new \cite{DBLP:journals/corr/GreffSKSS15}
% % 
% % % 
% % % Layered acting .. DOntcheva \cite{DBLP:journals/tog/DontchevaYP03}
% % 
% % 
% % Deep : 
% % Hintn 2006 \cite{DBLP:journals/cogsci/HintonOWT06}

\subsubsection{WP1. Kinesic component} 




This WP aims at creating multi-modal statistical models of an individual character's body movements from annotated data (e.g. motion capture data), to generate novel expressive animation suitable for dramatic performances. To do so we will tackle few difficult and open problems. 

First we will work at designing new full body controlers based on recent advances in statistical machine learning and on representation learning. We will focus on designing generic models allowing animation in many settings including  emotional state and actor's profile (morphology, expressivity level etc). We want in particular that, while the animation model will be learned from a limited number of actors' data, the controlers should be able to be remapped to other actors. 
Our idea is to build models that take as input few contextual variables that encode the setting (mood, actor's profile etc) as continuous input variables. 
% Designing models whose synthesized animation continuously vary with these contextual inputs naturally yield easy generalizing to unseen settings in the training set (e.g. particular combination of action, mood and actor's morphology). 
% Such generic models would allow learning the animation model for a particular action may be learned from all samples of this action whatever the mood ant the actor's morphology, and from all samples of any action performed with this particular mood.
In a first step we will extend our previous works on markovian contextual models \cite{Radenen2014, Ding2013, Ding2014} to full body animation. 
% These models are a variant of Hidden Markov Models (HMMs) that have shown strong potential for designing speech-based face controllers, where speech features are the contexual variables that infuence the way the face is animated. 
Next, we will investigate the use of continuous state space models and particularly of (deep) recurrent neural networks. Such models, and some of their variants, have been proposed recently for diffcult recognition tasks on complex signals (e.g. speech \cite{DBLP:journals/taslp/Abdel-HamidMJDPY14}). These frameworks are part of the representation learning line of work which brought impressive breakthrough in few machine learning tasks for signals, speech recognition \cite{DBLP:journals/spm/LingKZSSQMD15}†and computer vision tasks \cite{DBLP:conf/eccv/LeCun12,DBLP:journals/nn/Schmidhuber15}. 

% The main difficulty lies here in imagining ways to integrate the idea of using contextual variales in these models in order to integrate contextual information that would modify the behaviour of the models. 

Finally we plan to explore fully new alternative strategies such as using neuro muscular based models following ideas of deltalognormal models from \cite{DBLP:conf/icfhr/FischerPOS14}. Such a modeling allows recovering the sequence of neuromuscular commands that generated a handwritten gesture. Although these models led to impressive results in terms of modeling accuracy, these models have been applied to very simple gestures only and their ability to handle complex gestures is not certain. Yet if we were able to make them robust enough to provide valuable interprettation of complex motion, these models would bring in our opinion a very promising line of research for building new and natural motion controllers. 

% Next we will have to combine multiple animation models in order to take advantage of all existing works, in particular for the animation of the face. 
% Controlling a fully-articulated character is traditionally accomplished using a series of interwoven subcomponents responsible for various parts of the body. We will mainly distinguish between the body animation and the face animation.




% Thierry : parler plutot de 3 verrous scientifiques. We will organize the work in three tasks, the two first tasks are dedicated to modeling and synthesizing animations. We will distinguish between animating the whole body and animating the face which will be animated based on a given scenario but also based on the scenario from other actors to enable accurate interaction between actors (e.g. gazes). Why ? The last task is focused on making the models designed in thee two first tasks able to learn from few training data

% A first scientific lock lies in the design of generic body controllers able to synthesize the animation of the full body of a character for many settings (combination of action, emotion and actor's profile). 
% It is an elegant way for producing smooth animation of complex motions which usually requires artifical smoothing and postprocessing. It is also a relevant modeling framework for learning from limited datasets. 
% Indeed gathering a dataset including enough training samples for every combination of (action, emotion, actor profile) is unlikely. 
% Defining generic models should allow to maximize the exploitation of the training data for learning, which is a key issue here. 


% % The idea is to build models that take as input contextual variables that encode the setting in such a way that the animation model for a particular action may be learned from all samples of this action whatever the mood, and from all samples of any action performed with this particular mood.
% One main idea for doing so consists in extending to full body animation the idea of contextual models \cite{Radenen2014, Ding2013, Ding2014} which are a variant of Hidden Markov Models (HMMs) that have shown strong potential for designing face controllers. 
% Contextual HMMs are HMMs whose parameters (means of Gaussian distribution transition probabilities etc), are defined as a (learned) function of contextual variables. One Contextual HMM may be viewed as a continuum of HMMs, one model for every possible value of contextual variables.
% These models will serve as a baseline. Next, we will investigate the use of continuous state space models and particularly of (deep) recurrent neural networks. Such models have shown strong abilities for dealing with complex signals like speech and handwriting \cite{DBLP:journals/corr/Graves13}. 
% The main difficulty lies here in imagining ways to integrate the idea of using contextual variales in these models in order to integrate contextual information that would modify the behaviour of the models. 
% Finally we plan to explore alternative strategies such as using neuro muscular based models following ideas like the one of deltalognormal models from \cite{DBLP:conf/icfhr/FischerPOS14} which allow recovering the sequence of neuromuscular commands that generated a handwritten gesture. 

% We will also investigate new ways for parameterizing the animation model with few variables related to the emotional state of an actor and more generally to an actor‚Äôs profile. The difficulty here lies in the definition of defining the representation of an actor, we will attempt to learn it directly from data using recent techniques named as representation learning (Contardo 2014). 

A second lock will concern the animation of the face in dialogue situations. Given that we have worked previously with three complementary methods, we will focus here on how to mix our face animation models: a mocap based animation model \cite{DBLP:conf/icassp/DingRAP13,Ding2014}, a video-based animation model \cite{Barbulescu2014}, and the procedural animation model in the GRETA system  \cite{greta}. The main issue here will be to imagine efficient frameworks for inferring based on the scenario of the animation which model to use or combine to produce a final animation.

% The mocap based animation model will be easily built on previous works by the team \cite{YuThesis}. 
% Finally starting from previous work on visual prosody we will design the third model ... Ideally, this should be done without MOCAP data, using only audio and video processing, possibly enhanced with depth (kinect). To be continued (R√©mi)...We will explore strategies for optimally combining these three animation models...


Finally we want our animation models to be easily extendable to new activities, gestures and moods, by making them learnable from only few training samples. This  will allow enriching the system easily whitout a costly and tedious task of gathering a large corpus of training data as usually required in statistical machine learning. Learning statistical models from few samples is an open issue, it has ony been adressed in few studies for simple gestures like handwriting signals \cite{ArtieresPAMI07} \cite{NIPS2013_OneShot}.  We will go beyond these preliminary studies and will explore few ways that aim at favoring transfer from learning one action / mood model to learning another action / mood model. 
In particular we will explore strategies for modifying our generic controlers, e.g. based on recurrent neural networks or on contextual markovian models \cite{Radenen2014}, in order to enable transfer learning between action models so that a new action model can be learned with only few samples of this action. 
% Second we will mix the idea of continuous state space models and of dimension reduction to design low dimensional state space Reccurrent neural nets. Projecting the dynamics of an action move in such a low dimensional space should permit characterizing it much easier, enabling learning from few samples. 


% We will mainly investigate two approches for extending approaches developped in task T11 to enable learning from few samples. The first strategy consists in extending the idea of context variables that models of task T11 rely on in order to design a global model for all actions.
% In the case of markovian model for instance this means that instead of definig one model per action one could define a unique big markovian model where every state would stand for a particular position of the body and performing an action would correspond to following a path (i.e. a state sequence) in this big model. The model for a particular action would correspond then in a bundle of paths in this model.  Doing so one could expect that all the training data (whatever the action it corresponds to) could be exploited to learn all the states of this big markovian model, hence implementing some kind of transfer learning between actions. A new action would correspond then in a bundle of paths in this model and could be learnt from few samples only. Preliminary works that we did let us expect that such a strategy would work with statistical markovian models (Ding et al., 2013). In this case the above idea is implementing by introducing new contextual variables, which are indicators of the action to perform, 
% that modify the gaussian densities. We will first investigate this strategy deeper for contextual markovien models then we will extend this approach to neural networks. 
% 
% Second we will explore the use of using continuous state space models with a low dimensional state space (e.g. corresponding to the degree of freedom of body poses or to the neuro muscular commands) which should permit characterizing a particular motion or gesture as its dynamic in this latent space whose limited dimension would enable learning from few samples.  




\endinput