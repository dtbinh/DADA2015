

\subsubsection{WP1 Kinesics}


\begin{center}
\begin{tabular}{|p{3cm}|p{8cm}|}\hline
WP1 &  Kinesic component \\\hline
Responsable &  LIF  \\\hline
Participants &  Inria, Telecom ParisTech\\\hline
Duration  &  42 months  \\\hline
Objectives &  Develop new generic controlers of a single character alone based on precise spatio-temporal indications of his actions and mood. \\\hline
% Content &  \\\hline
Task 11 & Full body animation  ($T_0 \rightarrow  T_0+36$) \\\hline
Task12 &  Face and gesture animation ($T_0+12 \rightarrow T_0+36$) \\\hline
Task13 &  Learning from few samples ($T_0+18 \rightarrow  T0+42$) \\\hline
\end{tabular}
\end{center}



The goal of this WP is to develop new generic models able to produce animation of a single character. It includes designing animation models of a character realizing an action (walking, sitting etc) given a context that consists in a particular mood and in character profile (age, gender) as well as designing models for taking into account the interaction of the character with others (gaze, harm gesture). Moreover we will explore transfer learning strategies for learning these models from few training samples only to ease addition of new getures, actions and moods. 

The workpackage is divided into three subtasks: {\it animation of the full body of a character alone}, {\it animation of the face of a character engaged in dialogue}, and {\it strategies for learning models from few training data}.

 
% \paragraph{Task11} Full body animation


% Choosing representations for full body motion ; representation learning ; transfer learning ; parameterisation of kinesic components. This can include neuro-muscular variables ; also the choice of kinematic trees rooted at the head ; also the grouping of kinematic variables into synergies ; etc. 
% 
% One classical distinction is between world-frame positions and joint angle variables In our case, we are making a strong statement that we will  study kinesic variables for all joints relative to the rigid body frame associated with the actor. This could be the ground floor position of the actor plus a rigid body position associated with the actor?s head.  Thus proxemic variables could be footsteps and head movements ; kinesic variables could be all other joint angles or joint positions in world coordinates.


\paragraph{Task 1.1. Generic full body animation model}


% A first scientific lock lies in the design of generic body controllers able to synthesize the animation of the full body of a character for many settings (combination of action, emotion and actor's profile). 
% It is an elegant way for producing smooth animation of complex motions which usually requires artifical smoothing and postprocessing. It is also a relevant modeling framework for learning from limited datasets. 
% Indeed gathering a dataset including enough training samples for every combination of (action, emotion, actor profile) is unlikely. 
% Defining generic models should allow to maximize the exploitation of the training data for learning, which is a key issue here. 

The first task focuses on the design of original generic body controllers for full body animation of a character given a procedural animation scenario as output by WP2, i.e. a sequence of actions realized with a particular mood context and for a specific character profile (morphology, expressivity level). 
% We will first consider that there is one model per action and that the animation produced by such a model should take into account, as inputs, the mood context as well as the character profile context. 
% We will consider modeling frameworks that allow separating the mood components from the profile components so as to enable extrapolating moods to other profiles, and profiles to other moods. 
% To start we will consider that there is one model per action and that the animation produced by such a model should take into account, as inputs, the mood context as well as the character profile context, later on we will investigate one model for all settings (action, mood, charecter profile). 
We will investigate generic models whose behaviour depends on few contextual variables (e.g. mood components and actor's profile components) viewed as few inputs which influence the animation. 
We will explore three lines of research that we present below ordered by decreasing complexity (for us to make these work) and, as it might occur, of increasing potential. While the first one is the extension of our previous works, the two others are original lines of research for body controllers. 
% Designing models whose output (a synthesized animation) continuously varies with these contextual inputs naturally yield easy generalizing to unseen settings in the training set (e.g. particular combination of action, mood and actor's morphology). We plan to investigate the following lines of research:
% This will enable learning from a limited combination of (mood, profile) settings while allowing extrapolating to any other combination (mood, profile). 
% Whatever the models under investigation we will pay attention to focus on strategies that enable synthesizing smooth transition between successive actions, moods, or gestures. 


% Such generic models would allow learning the animation model for a particular action may be learned from all samples of this action whatever the mood ant the actor's morphology, and from all samples of any action performed with this particular mood.

% We will first explore extending prevous worls on contextual markovian models \cite{Radenen2014, Ding2013, Ding2014} which are a variant of Hidden Markov Models (HMMs) that are parameterized by contextual information (e.g. means of gaussian distributions of the HMM'states are made a function of contextual variables). This will require extending these models to deal with the full body.
% We will first explore extensions of previous promising works (e.g. Radenen 2014). Then we will explore and propose new ways for parameterizing the animation model with few variables related to the emotional state of an actor and more generally to an actor's profile. 

 
% \begin{itemize}  
%   \item 
We will start with an approach that we know better and that we are confident that it will provide interesting results. We will extend {\it contextual markovian models} whose parameters (means of Gaussian distribution, transition probabilities...), are parameterized by (i.e. defined as a function of) contextual variables. One such contextual HMM may be viewed as a continuum of HMMs, one model for every possible value of contextual variables. Recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers \cite{DBLP:journals/tog/LevineKTK10} and face controllers \cite{Radenen2014, Ding2013, Ding2014}. We will aim to generalize these works to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting objects, in a variety of expressions and moods. These models will serve as a baseline for evaluating new modeling approaches.



%   \item 
 Next, we will investigate the design of {\it (deep) neural networks} and of dynamic versions of these (i.e. recurrent neural nets) which have demonstrated strong abilities to model, to classify and to synthesize signals such as speech or handwriting \cite{DBLP:journals/taslp/Abdel-HamidMJDPY14,DBLP:journals/corr/abs-1303-5778,DBLP:conf/interspeech/Abdel-HamidDYJ13,DBLP:journals/spm/LingKZSSQMD15, DBLP:conf/nips/GravesS08,DBLP:journals/corr/Graves13}. These models are related to what is called {\it representation learning} and {\it deep learning} which emerged in the last few years as a key topic in the machine learning community \footnote{See the recently born ICLR conference on Learning Representations at http://www.iclr.cc/} \cite{DBLP:journals/corr/ContardoDAG13}.  One main difficulty will be to integrate the use of contextual information as input in order to modify the behaviour of the models. We plan to extend the principle of contextual markovian models to neural nets by investigating ideas like designing bilinear layers in the neural net where the weights of a layer could be defined as a function of the contextual input, inspired by works like \cite{DBLP:conf/mm/ZhongLL11, DBLP:journals/pami/HutchinsonDY13}.
%   \item 

  At last we will investigate using {\it low dimensional state space models} and in particular neuro muscular based models following ideas like \cite{DBLP:conf/icfhr/FischerPOS14} which aims at recovering from a handwritten signal the sequence of neuromuscular commands that generated the handwriting signal. The underlying idea here is to exploit such models in order to work in a new representation space, the space of neuromuscular commands that generate motion, rather than on the observed motion itself. It is expected that working in that space will allow designing much more natural controllers.
  Although such models have not been used to model complex gestures up to now it is expected that they could be robust enough to provide good estimation of the command sequence. The main advantage of such a change of representation space is an expected reduction of the dimension of this space (as in \cite{DBLP:journals/pami/WangFH08}), enabling easier learning from few samples and transfer learning (as will be investigated in \textbf{task 1.3}). 
%   This would be linked to (Gaussian process reference).
% 
% \end{itemize}


\paragraph{Task 1.2. Combining models for face animation } 

The second task focuses on learning models of facial expressions in dialogue situations. We will start from three available face animation models in the consortium:  a mocap based animation model \cite{DBLP:conf/icassp/DingRAP13,Ding2014}, a video-based animation model \cite{Barbulescu2014}, and the procedural animation model in the GRETA system  \cite{greta}. The main issue here will be to imagine efficient frameworks for inferring based on the scenario of the animation which model to use or combine to produce a final animation.

All of our three controllers have pros and cons. While statically-driven models are more prone to produce natural looking animation, cognitive
models capture more precisely the semantic emotional behaviors to communicate. These latter ones are often event-driven; that is they compute a behavior only when a given communicative function is specified. Statically driven models produce animation continuously that captures the communicative colour of the message to convey but they have difficulty to compute behaviors which have specific meaning. As a result, virtual agents driven by cognitive-like system are able to convey more precise displays while those driven by statistical models look more natural and lively \cite{DBLP:conf/iva/LeeM12}.

We will explore few ways to combine animation models which remains an open question today, be it for animating the face or the full body. We will implement our solutions within the Greta framework where communicative intentions and emotions are represented with the FLM language while multimodal behaviors with BML \cite{DBLP:conf/iva/VilhjalmssonCCCKKMMMPRTWW07}. The merge of multiple animation models may then be performed as a weighted blend of the animations produced where the weights might be context dependent and tuned either manually or automatically, alike in \cite{DBLP:journals/tvcg/ShoulsonMKB14}. Alternatively the animation models may be merged earlier, when deciding which kind of motion to launch, or may have asymetric role. For instance, the procedural animation model (or semantically-driven) might act as the main animation model and use when neccessary animations produced by the other models.




% 
% 
% This subtask focuses on learning models of gesture and facial expressions in dialogue situations. Based on previous work on  visual prosody, we would like to learn joint models of gesture and  speech prosody. Ideally, this should be done without MOCAP data, using only audio and video processing, possibly enhanced with depth (kinect). To be continued (RÃ©mi)...

\paragraph{Task 1.3. Learning from few samples}
This last task concerns strategies that we will design for learning our controllers from few samples. The idea is that while task 1 focuses on how to design accurate and natural controllers without caring about the ability of these models to be learned in practice, when training samples are not numerous, this task aims at proposing strategies to slightly modify these controllers for making them learnable in realistic settings.
We will mainly investigate two approches. The first strategy consists in extending the idea of context variables that models of task T1.1 rely on in order to design a global model for all actions. In the case of markovian models for instance this means that instead of defining one model per action one could define a unique global markovian model where every state would stand for a particular position of the body and performing an action would correspond to following a path (i.e. a state sequence) in this big model. 
One idea is to make transition probablities dependent on the action to perform so that an action model would correspond to a bundle of paths only in this model.  Doing so one could expect that all the training data (whatever the action it corresponds to) could be exploited to learn all the states of this big markovian model, hence implementing some kind of transfer learning between actions.  Preliminary works that we did let us expect that such a strategy would work with statistical markovian models \cite{DBLP:conf/icassp/DingRAP13, Radenen2014}. We will then extend this kind of ideas to recurrent neural networks and continuous state space models. Indeed in such models, a particular motion correspond to a dynamic behaviour in the latent space. Relying on a low dimensional state space (e.g. corresponding to the degrees of freedom of body poses or to the neuro muscular commands) will mechanically reduce the dynamic model complexity (e.g. number of parameters) and would favor learning from few samples.


\begin{tabular}{|p{3cm}|p{10cm}|p{1.5cm}|}\hline
Deliverables & Name and content  & Date  \\\hline
L1.1  & Report on the state of the art for statistical models for animation synthesis &  $T_0+12$ \\\hline
L1.2  & First version of the models : Prototype (software) and its documentation (Report on the models developed) & $T_0+24$ \\\hline
L1.3  & Second version of the models : Prototype (software) and its documentation (Report on the models developed)  & $T_0+36$ \\\hline
\end{tabular}

\paragraph{Partners' roles}

LIF is the responsible partner for this work package. Inria and LITC will take part in the development of expressive text-to-speech technologies and audio-visual prosodyfor task 1.2, using their respective previous work in this areas. Inria and LITC will also bring all required expertise in 3D animation to ensure the success of  tasks 1.1 and 1.3 and we will be targeting joint research work to ensure that the resulting work can be published in major computer graphics conferences, as well as in machine learning conferences.

\paragraph{Risks}

The risks are limited with respect to the availability of datasets. All along the project we will rely as much as possible on existing datasets. For instance Mocap data of considered actions have already been recorded by C. Pelachaud within the project Feder Anipev (http://www.anipev.com/) and the corpus EMILYA (EMotional body expressIon in daiLY Actions databaseBodily Emotional Actions Behavior) has been created by one of the partner (Fourati, 2014).
% is constituted of 7 actions performed by 11 actors with 8 emotions. The actions encompass everyday actions such as walking, carrying an object, and sitting. 
% The emotions cover the positive and negative spectrum.



\endinput