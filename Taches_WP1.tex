

\subsubsection{WP1 Kinesics}


\begin{center}
\begin{tabular}{|l|l|}\hline
WP1 &  Kinesic component \\\hline
Responsable &  ECM  \\\hline
Participants &  Inria, Telecom ParisTech\\\hline
Duration  &   \\\hline
Objectives &   \\\hline
Content &  \\\hline
Task 11 & Full body animation  \\\hline
Task12 &  Interaction animation \\\hline
Task13 &  Learning from few samples  \\\hline
\end{tabular}
\end{center}



The aim of this WP is to develop new generic models able to produce animation of a single character. It includes designing animation models of a character realizing an action (walking, sitting etc) given a context that consists in a particular mood and in character profile (age, gender) as well as designing models for taking into account the interaction of the character with others (gaze, harm gesture). Moreover it will explore the ability to extend these models in order to deal with only few training data by relying on transfer learning strategies.

The workpackage is divided into three subtasks which are dedicated to the animation of the full body of a character, to the animation of specific parts of its body which are engaged in an interaction with another character (mainly the face) by combining few animation models, and to the specific strategies that will be explored for learning such models from few training data.

% \paragraph{Task11} Full body animation


% Choosing representations for full body motion ; representation learning ; transfer learning ; parameterisation of kinesic components. This can include neuro-muscular variables ; also the choice of kinematic trees rooted at the head ; also the grouping of kinematic variables into synergies ; etc. 
% 
% One classical distinction is between world-frame positions and joint angle variables In our case, we are making a strong statement that we will  study kinesic variables for all joints relative to the rigid body frame associated with the actor. This could be the ground floor position of the actor plus a rigid body position associated with the actor?s head.  Thus proxemic variables could be footsteps and head movements ; kinesic variables could be all other joint angles or joint positions in world coordinates.


\paragraph{Task 1.1. Generic full body animation model}

We will first focus on the design of generic body controllers able to synthesize the animation of the full body of a character (through the sequence of mocap representation) for a given procedural animation scenario as output by WP2, i.e. a sequence of actions realized with a particular mood context and for a specific character profile (morphology, expressivity level). 
% We will first consider that there is one model per action and that the animation produced by such a model should take into account, as inputs, the mood context as well as the character profile context. 
% We will consider modeling frameworks that allow separating the mood components from the profile components so as to enable extrapolating moods to other profiles, and profiles to other moods. 

To start we will consider that there is one model per action and that the animation produced by such a model should take into account, as inputs, the mood context as well as the character profile context, later on we will investigate one model for all settings (action, mood, charecter profile). We will investigate modeling frameworks that allow taking into account the contextual variables (e.g. mood components and profile components) as few inputs which are mixed to produce an animation. This will enable learning from a limited combination of (mood, profile) settings while allowing extrapolating to any other combination (mood, profile). 
Whatever the models under investigation we will pay attention to focus on strategies that enable synthesizing smooth transition between successive actions, moods, or gestures. 
% We will first explore extending prevous worls on contextual markovian models \cite{Radenen2014, Ding2013, Ding2014} which are a variant of Hidden Markov Models (HMMs) that are parameterized by contextual information (e.g. means of gaussian distributions of the HMM'states are made a function of contextual variables). This will require extending these models to deal with the full body.
% We will first explore extensions of previous promising works (e.g. Radenen 2014). Then we will explore and propose new ways for parameterizing the animation model with few variables related to the emotional state of an actor and more generally to an actor's profile. 
We plan to investigate the following lines of research:
 
% \begin{itemize}  
%   \item 
  Firstly we will investigate {\it contextual markovian models} where gaussian probability density functions associated to states are parameterized by (i.e. defined as a function of) contextual observation (mood and profile information). Recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers (Levine 2010) and face controllers \cite{Radenen2014, Ding2013, Ding2014}. We will aim to generalize these works to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting objects, in a variety of expressions and moods. These models will serve as a baseline for evaluating new modeling approaches.

%   \item 
  Second, we will investigate the use of {\it (deep) neural networks} and of dynamic versions of these (i.e. recurrent neural nets) which have demonstrated strong abilities to model, to classify and to synthesize complex signals such as speech or handwriting \cite{HintonDeepSpeech, Deng_Speech_Reco, DBLP:conf/nips/GravesS08, DBLP:journals/corr/Graves13}. 
  These models are related to what is called {\it representation learning} which emerged in the last few years as a key topic in the machine learning community \footnote{See the recently born ICLR conference on Learning Representations at http://www.iclr.cc/} (Contardo 2014). 
  One main difficulty will be to integrate the use of contextual information as input in order to modify the behaviour of the models. We plan to extend the principle of contextual markovian models to neural nets by investigating ideas like designing bilinear layers in the neural net where weights could be defined as a function of the contextual input, inspired by works like \cite{DBLP:conf/mm/ZhongLL11, DBLP:journals/pami/HutchinsonDY13}.
%   \item 
  At last we will investigate {\it low dimensional state space models} such as neuro muscular based models following ideas like \cite{DBLP:conf/icfhr/FischerPOS1} which aims at recovering from a handwritten signal the sequence of neuromuscular commands that generated the handwriting signal. The underlying idea here is to exploit such models in order to work in a new representation space, the space of neuromuscular commands that generate motion, rather than on the observed motion itself. 
  Although such models have not been used to model complex gestures up to now it is expected that they could be robust enough to provide good estimation of the command sequence. The main advantage of such a change of representation space is an expected reduction of the dimension of this space (as in \cite{DBLP:journals/pami/WangFH08}), enabling easier learning from few samples and transfer learning (as will be investigated in task 13).
%   This would be linked to (Gaussian process reference).
% 
% \end{itemize}


\paragraph{Task 1.2. Combining models for face animation } 

The second task focuses on learning models of gesture and facial expressions in dialogue situations. It is dedicated to the combination of animation models, whch is a diifuckt and open question, with a focus on the animation of the face. We will start from available face animation models in the consortium: a mocap based animation \cite{Ding2013}, a video-based animation \cite{TheseINRIA}, and a procedural animation \cite{Greta} (integrated in the GRETA system). 

All of these models types have pros and cons. While statically-driven models are more prone to produce natural looking animation, cognitive
models capture more precisely the semantic emotional behaviors to communicate. These latter ones are often event-driven; that is they compute a behavior only when a given communicative function is specified. Statically driven models produce animation continuously that captures the communicative colour of the message to convey but they have difficulty to compute behaviors which have specific meaning. As a result, virtual agents driven by cognitive-like system are able to convey more precise displays while those driven by statistical models look more natural and lively \cite{DBLP:conf/iva/LeeM12}.

We will explore ways to combine few such animation models which remains an open question today, be it for animating the face ot the full body \cite{...}. We will explore strategies and implement these within the Greta framework where communicative intentions and emotions are represented with the FLM language while multimodal behaviors with BML \cite{DBLP:conf/iva/VilhjalmssonCCCKKMMMPRTWW07}. The merge of multiple animation models may be performed as a weighted blend of the animations produced where the weights might be context dependent and tuned either manually or automatically, alike in \cite{DBLP:journals/tvcg/ShoulsonMKB14}. Alternatively the animation models may be merged earlier, when deciding which kind of motion to launch, or may have asymetric role. For instance, the procedural animation model (or semantically-driven?) might act as the main animation model and use when neccessary animatyions produced by the other models.




% 
% 
% This subtask focuses on learning models of gesture and facial expressions in dialogue situations. Based on previous work on  visual prosody, we would like to learn joint models of gesture and  speech prosody. Ideally, this should be done without MOCAP data, using only audio and video processing, possibly enhanced with depth (kinect). To be continued (RÃ©mi)...

\paragraph{Task 1.3. Learning from few samples}

We will mainly investigate two approches for extending approaches developped in task T11 to enable learning from few samples. The first strategy consists in extending the idea of context variables that models of task T11 rely on in order to design a global model for all actions.
In the case of markovian model for instance this means that instead of definig one model per action one could define a unique global markovian model where every state would stand for a particular position of the body and performing an action would correspond to following a path (i.e. a state sequence) in this big model. 
Making transition probablities dependent on the action to perform such a big model would be instaiated as an action model by considering a bundle of paths only in this model.  Doing so one could expect that all the training data (whatever the action it corresponds to) could be exploited to learn all the states of this big markovian model, hence implementing some kind of transfer learning between actions. 
A new action would correspond then in a bundle of paths in this model and could be learnt from few samples only. Preliminary works that we did let us expect that such a strategy would work with statistical markovian models (Ding et al., 2013). In this case the above idea is implementing by introducing new contextual variables, which might be at the simplest on-hot indicators 
of the action to perform (a vector with zeros everywhere but at the position of the action number), 
that modify the gaussian densities. We will first investigate this strategy deeper for contextual markovien models then we will extend this approach to neural networks...

Second we will explore the use of using continuous state space models with a low dimensional state space (e.g. corresponding to the degree of freedom of body poses or to the neuro muscular commands) which should permit characterizing a particular motion or gesture as its dynamic in this latent space whose limited dimension would enable learning from few samples...

\paragraph{Deliverables}

bla bla bla

\begin{tabular}{|l|l|l|}\hline
Deliverables & Name and content  & Date  \\\hline
L1.1  & Report on the state of the art for statistical models for animation synthesis & \\\hline
L1.2  &  & \\\hline
L1.3  &  & \\\hline
\end{tabular}

\paragraph{Partners' roles}

bla bla

\endinput