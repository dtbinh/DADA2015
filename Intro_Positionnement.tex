\section{R\'esume de la proposition de projet / Executive summary}

\begin{xcomment}  
Recopier le r\'esum\'e utilis\'e dans le document administratif et financier.
\end{xcomment}








\subsection{Contexte et enjeux \'economiques et soci\'etaux / Context, social and economic issues}
\begin{xcomment}  
D\'ecrire le contexte \'economique, social, r\'eglementaire' dans lequel se situe le projet en pr\'esentant une analyse des enjeux sociaux, \'economiques, environnementaux, industriels' Donner si possible des arguments chiffr\'es, par exemple, pertinence et port\'ee du projet par rapport à la demande \'economique (analyse du march\'e, analyse des tendances), analyse de la concurrence, indicateurs de r\'eduction de coûts, perspectives de march\'es (champs d'application, '). Indicateurs des gains environnementaux, cycle de vie'
\end{xcomment}

Animating virtual agents with expressivity is a big challenge for the entertainment industries (video games, movie industry) that rely mainly on motion capture data which allows them to produce rich and subtle motion but with a high cost in time and finance. On the other hand, technology for interactive agents uses mainly procedural approach. While such approach allows modulating in real-time agents' motion and its quality, the results are still far from being natural and realistic. Lately statistical approaches have been developed. They are promising as they produce animations captur-ing naturalness and richness of human motion. However the control of such animation technique is still an issue and its extension to a large range of motion activities is also an important challenge.


DADA aims to bridge the gap between those previous techniques by proposing a general framework for combining them into a unified interface. A desirable outcome of the project will be a completely novel interaction model for rehearsing with virtual actors and incrementally building complex multi-actor performances with multiple layers of 3D animation. Thus DADA fits the component Information and Communication Society; it is also in line with at least two axes of the ANR call.
First axis: Le num\'erique au service des arts, du patrimoine, des industries culturelles et \'editoriales (3.7.1.3). There are several potential applications of DADA on top of the proposed virtual theater. The creation of expressive virtual charac-ters can be used in video games, especially for NPC (non-player characters), and in serious games. Indeed being able to simulate the motion of a virtual actor with different expressivities and for different morphologies while maintaining a high level of naturalness and lifelikeness will be a big benefit in time and money.


Second axis: Interactions des mondes physiques, de l'humain et du monde num\'erique (3.7.2.4). The outcome of DADA
will benefit the creation of virtual agents, either autonomous or controlled by humans. These agents ought to display a
large variety of communicative and emotional expressions toward human interactants as well as to perform many actions with objects in their virtual environment. Enhancing, in quantity and expressivity, the behaviors of virtual agents is one of the challenges of DADA that falls under the axis.

Related projects: There exist several large European projects that are related to DADA research themes. However, to our knowledge, none covers our research question of building expressive animation with different levels of control. We can name the NoE IRIS on story-telling, the IP Companions on dialog virtual agents, the IP REVERIE on modeling virtual characters in highly immersive virtual environment, and the STREP Ilhaire aims to simulate laughing agent using data-driven and motion graph approaches. On the National side, we can name the Feder project Anipev in which the database Emilya has been captured.

Several recent projects have been devoted to the interface between computers and theatre, e.g. ANR VIRAGE (a generic architecture for controlling lighting and music during theatre production), ANR OSSIA (authoring tools for writing interactive, multimedia scenarios) and ANR INEDIT (INteractivit\'e dans l'Ecriture De l'Interaction et du Temps). ANR Spectacle-en-ligne(s) is a SHS CORPUS project dedicated to capturing, indexing and annotating 200 hours of theatre rehearsals recorded in high-definition video. Those projects focus on the interaction of computer systems with real actors. In contrast, DADA will focus on the core issue of directing virtual.

Despite considerable academic research, few procedural animation systems have become commercially available in recent years. Euphoria by Natural Motion is a real-time procedural animation engine, which has been used in Grand Theft Auto 4 and other games. However, actions and expressions are difficult to control. Xtranormal Technologies was an online service for quickly creating 3-D animations from dialogues decorated with stage directions, using a proprietary procedural animation engine limited to non-expressive behaviors.  Actor Machines is a company created by Ken Perlin to commercialize packages of trained virtual actors  with a large range of actions and expressions, which has not delivered any product yet.




\subsection{Positionnement du projet / Position of the project}
\begin{xcomment}  
Pr\'eciser :
positionnement du projet par rapport au contexte d\'evelopp\'e pr\'ec\'edemment : vis- à-vis des projets et recherches concurrents, compl\'ementaires ou ant\'erieurs, des brevets et standards'
indiquer si le projet s'inscrit dans la continuit\'e de projet(s) ant\'erieurs d\'ejà financ\'es par l'ANR. Dans ce cas, pr\'esenter bri\`evement les r\'esultats acquis,
positionnement du projet par rapport aux axes th\'ematiques de l'appel à projets,
positionnement du projet aux niveaux europ\'een et international.
\end{xcomment}

Creating believable, human-like performances by virtual actors is an important problem in many digital storytelling applications, e.g. creating non-player characters (NPC) for video games, creating expressive avatars in next-generation virtual worlds, populating movies and architectural simulations with background characters and crowds, creating be-lievable virtual tutors and coaches in educational serious games, and creating believable characters for interactive fiction and interactive drama (Tannenbaum 2014).

A desirable feature for such applications is the ability to create virtual actor performances which are both expressive and controllable. Motion capture actors are expressive, but once recorded, their performances cannot easily be controlled, edited or modified. As a result, game companies ought to get engaged in extensive motion capture sessions of all actions and moods of all characters in every new game they create. On the other end of the spectrum, procedural 3D animation can be controlled in every detail using sophisticated programming techniques, but they fall short of providing the level of expression required for conveying the subtle inflexions of human-like performances.

Character animation has been tackled through various approaches in the past. To name a few, chosen among those that are directly related to DADA, we can cite: embodied conversational agents (ECA), ie autonomous virtual characters (Cassell 2000); statistical models learned from motion capture examples (Lee 2002); physically-based animation (Liu 2006); and speech-driven animation (Ding 2013). Very few attempts have tried to merge these various approaches into a single model offering on one hand expressive animation and on the other hand high control over the animation.
In order to make progress in the field, we propose to shift the focus from autonomous characters to autonomous actors. Autonomous characters (such as The Sims) make decisions based on AI models of their personality and goals. In con-trast, autonomous actors follow a precise script, written by the director. Their autonomy is therefore limited to perform-ing a precise sequence of actions as a result of various cues  written in the script. Creating such performances proce-durally using autonomous actors is a valuable goal because it would make it possible for each performance to be unique, which is widely regarded as an important quality to ensure liveliness and immersion, while maintaining a high level of directorial control. Merging both approaches would allow creating autonomous actors able to follow a script (specified in high-level command-like language) that give the main directions the actors ought to follow while adapting their behaviors autonomously to the virtual environment they are placed in that 
includes objects and other actors.

The goal of the DADA project is to design, implement and evaluate novel interfaces for directing expressive, autonomous virtual actors, borrowing from established theatre practices. We will combine fundamental re-search in 3D animation, machine learning and intelligent agent programming to leverage motion capture data sets of professional actors into a virtual theatre company of synthetic actors with acting skills, i.e. ability to respond to a director's instructions and to perform together on a virtual stage. Virtual theatre will be used as a test application for obvious extensions to other digital storytelling applications.

To reach this ambitious goal, DADA will learn parameterized models of actor's movements and gestures from existing annotated motion capture databases of actor performances; and create intuitive authoring tools for creating a script of actions and cues in a machine-readable format suitable to real-time control of the virtual actors. More precisely, the academic partners of the project will engage fundamental research along two main directions:

\begin{enumerate}
\item	Animating autonomous actors procedurally. A key idea in DADA is to separate the animation model into a proxe-mic component regulating how actors interact with each other and the audience, and a kinesic component regulating how actors use their body language to communicate moods and expressions (Tannenbaum 2014). The proxemic component of animation will drive the positions and orientations of actors on the stage as well as their gaze direc-tions. This component will be driven by a model encompassing the social relations between and the emotional states of the autonomous actors. The kinesic component of animation will drive all other degrees of freedom of the virtual actors. This component will be driven by parametric statistical models trained from an existing motion cap-ture data-set. The separation between the two components is expected to yield important benefits in terms of ex-pressivity and composability.
\item 	Synchronizing virtual actors to a single story-line using a story-driven architecture of actors following a scripted sequence of instructions (Pinhanez 2000). In contrast to previous works, which used programming languages (Ma-teas 2002), we will investigate multimodal interfaces offering directorial control in a high-level, pseudo-natural language familiar to the director. The language will be compiled internally to a finite-state machine representation controlling the real-time execution of the autonomous actors. 
\end{enumerate}


All developments will be validated by experiments with the theatre department of Paris 8, under the supervision of Georges Gagner\'e. Starting from a selection of play scripts in various genres and with increasing complexity, theatre experts will use the DADA tools to create virtual theatre performances in the Unity game engine, including stage movements and actions (entering, exiting, sitting down, standing up, taking and putting objects on the stage); body language expression of the personalities, moods and emotions of the characters; and believable gaze, proxemics and action/reaction behaviors between actors.

The expected results of DADA will be (1) a virtual theatre company of autonomous actors with a large vocabulary of expressive animation skills; and (2) a prototype system for directing arbitrary dramatic plays, amenable to a variety of digital storytelling applications. Results will be integrated into Unity3D which is already used by the GRETA plat-form at Telecom ParisTech and the virtual cinematography framework developed by the IMAGINE team at Inria. Re-sults will be used at University of Marseille for building a pivot actor model allowing the retargeting of the DADA actors to actors with different morphologies and styles. Results will be used by Paris 8 as a virtual rehearsal space for theatre productions involving real actors interacting with digital actors, and as a platform for publishing digital dramatic performances online. If applicable, results will also be patented and exploited by the three academic partners, targeting commercial applications such as video games, digital storytelling, virtual 
worlds and movie previz.

Budget: We request a financial aid of 450 K\euro for 3 PhD students (360 K\euro), 1 post-doc at Paris 8 (40 K\euro), computer hardware and software (10 k\euro), travel expenses (40 K\euro). The project duration should be 42 months in order to develop a functional prototype and to use it to animate several play scripts.



\subsection{Etat de l'art / State of the art}
\begin{xcomment}  
Pr\'esenter un \'etat de l'art national et international, en dressant l'\'etat des connaissances sur le sujet. 
Faire apparaître d'\'eventuelles contributions des membres de la proposition de projet à cet \'etat de l'art.
Faire apparaître d'\'eventuels r\'esultats pr\'eliminaires. 
Inclure les r\'ef\'erences bibliographiques n\'ecessaires au § 7.
\end{xcomment}



\subsubsection{WP1. Related works}

Animation of an avatar is usually tackled by working separately on the full body animation model on the one hand and on the face (and gesture) animation model on the other hand (since the latter animation strongly depends on the dialogue the avatar is engaged in), where the animation produced by the two models are merged to produce a final complete animation \cite{DBLP:journals/tvcg/ShoulsonMKB14}. 

Full body kinematic animation (or control) consists in animating the full body of an avatar while he is performing actions such as walking, dancing, sitting etc. Although there has been lots of work on this subject it is still a challenging problem due to the high dimensionality of the character's configuration. Data-driven approaches are very popular here and make of use motion-capture data to learn animation models which, once learned, may be used to animate a virtual character to perform a given task. 
Many systems have been proposed for producing animation models and controlers, they usually are based on statistical models such as Hidden Markov Models (HMMs) \cite{DBLP:journals/tog/LevineTK09} and Conditional Random Fields (CRFs) \cite{DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14}. Most accurate methods exploit a large dataset of motions where one can synthesize a complete motion sequence corresponding to a particular task by using warping or blending strategies of motions in the training set \cite{WitkinAndPopovic1995}. Locomotion controllers have been proposed that concatenate motion
clips from a motion capture dataset to produce an animation that is smooth [Treuille et al. 2007; Mc-Cann and Pollard 2007]. High-quality kinematic controllers have been built from this idea by using a {\it motion graph}, which is a graph structure that describe how clips from a dataset can be reordered into new motions \cite{LeeEtAl2002}. 
While locomotion controllers are driven
by direct high-level commands (such as desired movement direction), no such clear control signal is available for body language. To animate the face, and accompanying arm gestures, many works have focused on developing specific animation models based on a dialogue related input, either speech, text or prosody features \cite{DBLP:journals/tog/LevineTK09, DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14, NOUS}. 
At the end, recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers (Levine 2010) and face controllers (Ding et al., 2013). 
% We will aim to generalize this previous work
% to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting
% objects, in a variety of expressions and moods.
Yet all these statistical approaches require large annotated datasets to work well.

Thereby these approaches do not easily work with small training sets which is a key issue, as stressed for instance in \cite{DBLP:journals/tog/LevineWHPK12}, since first it requires considerable effort and time to build large datasets, and second because many applications demand unique motion styles and require their own datasets. This has led a number of researchers to put the effort on designing models that may be easily learned from a few samples. One main approach for doing so lies in the use (or learning) of a continuous state space to represent the data, making learning in this low dimensional space much easier  \cite{DBLP:journals/tog/LevineWHPK12, DBLP:conf/atal/ChiuM14}. A relevant technology for this are Gaussian Process which have been extended for dealing wiuth dynamic data in \cite{DBLP:journals/pami/WangFH08}.

These latter models are not far from recurent neural networks, and to Long Short Term Memory neural networks in particular \cite{DBLP:conf/nips/HochreiterS96, DBLP:journals/corr/GreffSKSS15}, that have been shown recently to work well for complex signals such as speech and handwriting, for recognition tasks \cite{DBLP:conf/nips/GravesS08} as well as for synthesis tasks \cite{DBLP:journals/corr/Graves13}. These models are part of a current trend in machine  learning called representation learning (see the recently born conference ICLR at http://www.iclr.cc/) which aims at discovering relevant and usually low dimensional representation of the data under investigation (the pionner work of this domaine is the one by G. Hinton in Science \cite{DBLP:journals/cogsci/HintonOWT06}.




\nocite{*}

\subsection{Objectifs et caract\`ere ambitieux/novateur du projet / Objectives, originality and novelty of the project}
\begin{xcomment}  
D\'ecrire les objectifs du projet et d\'etailler les verrous scientifiques et techniques à lever par la r\'ealisation du projet. Insister sur le caract\`ere ambitieux et/ou novateur de la proposition.
D\'ecrire \'eventuellement le ou les produits finaux d\'evelopp\'es, pr\'esenter les r\'esultats escompt\'es en proposant si possible des crit\`eres de r\'eussite et d'\'evaluation adapt\'es au type de projet, permettant d'\'evaluer les r\'esultats en fin de projet.
\end{xcomment}  


\endinput
