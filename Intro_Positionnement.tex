\section{R\'esume de la proposition de projet / Executive summary}

\begin{xcomment}  
Recopier le r\'esum\'e utilis\'e dans le document administratif et financier.
\end{xcomment}

Character animation has been tackled through various approaches in the past. To name a few, chosen among those that are directly related to DADA, we can cite: embodied conversational agents (ECA); statistical models learned from motion capture examples; physically-based animation; and speech-driven animation. Very few attempts have tried to merge these various approaches into a single model offering on one hand expressive animation and on the other hand high control over the animation. In order to make progress in the field, we propose to shift the focus from autonomous characters to autonomous actors. Autonomous characters make decisions based on AI models of their personality and goals. In contrast, autonomous actors follow a precise script, written by the director, while adapting their behaviors autonomously to the virtual environment they are placed in that includes objects and other actors. 

The goal of the DADA project is to design, implement and evaluate novel interfaces for directing expressive, autonomous virtual actors, borrowing from established theatre practices. We will combine fundamental research in 3D animation, machine learning and intelligent agent programming to leverage motion capture data sets of professional actors into a virtual theatre company of synthetic actors with acting skills, i.e. ability to respond to a director's  instructions and to perform together on a virtual stage. Virtual theatre will be used as a test application for obvious extensions to other digital storytelling applications.

To reach this ambitious goal, DADA will learn parameterized models of actor movements and gestures from existing annotated motion capture databases of actor performances; and create intuitive authoring tools for creating a script of actions and cues in a machine-readable format suitable to real-time control of the virtual actors. More precisely, the academic partners of the project will engage fundamental research along two main directions:

\begin{enumerate} 
\item Animating autonomous actors procedurally. A key idea in DADA is to separate the animation model into a proxemic component regulating how actors interact with each other and the audience, and a kinesic component regulating how actors use their body language to communicate moods and expressions \cite{Tanenbaum2014}. The proxemic component of animation will drive the positions and orientations of actors on the stage as well as their gaze directions. This component will be driven by a model encompassing the social relations between and the emotional attitudes of the autonomous actors. The kinesic component of animation will drive all other degrees of freedom of the virtual actors. This component will be driven by parametric statistical models trained from an existing motion capture data-set. The separation between the two components is expected to yield important benefits in terms of expressivity and composability.

\item Synchronizing virtual actors to a single story-line using a story-driven architecture of actors following a scripted sequence of instructions. In contrast to previous works, which used programming languages, we will investigate multimodal interfaces offering directorial control in a high-level, pseudo-natural language familiar to the director. The language will be compiled internally to a finite-state machine representation controlling the real-time execution of the autonomous actors. 
\end{enumerate}


All developments will be validated by experiments with the theatre department of Paris 8. Starting from a selection of play scripts in various genres and with increasing complexity, theatre experts will use the DADA tools to create virtual theatre performances in the Unity game engine, including stage movements and actions (entering, exiting, sitting down, standing up, taking and putting objects on the stage); body language expression of the personalities, moods and emotions of the characters; and believable gaze, proxemics and action/reaction behaviors between actors.


\section{Introduction et positionnement / Introduction and positioning}

\subsection{Contexte et enjeux \'economiques et soci\'etaux / Context, social and economic issues}
\begin{xcomment}  
D\'ecrire le contexte \'economique, social, r\'eglementaire' dans lequel se situe le projet en pr\'esentant une analyse des enjeux sociaux, \'economiques, environnementaux, industriels' Donner si possible des arguments chiffr\'es, par exemple, pertinence et port\'ee du projet par rapport √† la demande \'economique (analyse du march\'e, analyse des tendances), analyse de la concurrence, indicateurs de r\'eduction de co\^uts, perspectives de march\'es (champs d'application, '). Indicateurs des gains environnementaux, cycle de vie'
\end{xcomment}

Creating believable, human-like performances by virtual actors is an important problem in many digital storytelling applications, e.g. creating non-player characters (NPC) for video games, creating expressive avatars in next-generation virtual worlds, populating movies and architectural simulations with background characters and crowds, creating believable virtual tutors and coaches in educational serious games, and creating believable characters for interactive fiction and interactive drama. A desirable feature for such applications is the ability to create virtual actor performances which are both expressive and controllable. Motion capture actors are expressive, but once recorded, their performances cannot easily be controlled, edited or modified. As a result, game companies ought to get engaged in extensive motion capture sessions of all actions and moods of all characters in every new game they create. On the other end of the spectrum, procedural 3D animation can be controlled in every detail using sophisticated programming techniques, but they fall short of providing the level of expression required for conveying the subtle inflexions of human-like performances.

Character animation has been tackled through various approaches in the past. To name a few, chosen among those that are directly related to DADA, we can cite: embodied conversational agents (ECA), ie autonomous virtual characters \cite{Cassell2000}; statistical models learned from motion capture examples \cite{Lee2002}; physically-based animation \cite{Liu2006}; and speech-driven animation \cite{Ding2013}. Very few attempts have tried to merge these various approaches into a single model offering on one hand expressive animation and on the other hand high control over the animation. In order to make progress in the field, we propose to shift the focus from autonomous characters to autonomous actors. Autonomous characters (such as The Sims) make decisions based on AI models of their personality and goals. In contrast, autonomous actors follow a precise script, written by the director. Their autonomy is therefore limited to perform-ing a precise sequence of actions as a result of various cues  written in the script. Creating such performances procedurally using autonomous actors is a valuable goal because it would make it possible for each performance to be unique, which is widely regarded as an important quality to ensure liveliness and immersion, while maintaining a high level of directorial control. Merging both approaches would allow creating autonomous actors able to follow a script (specified in high-level command-like language) that give the main directions the actors ought to follow while adapting their behaviors autonomously to the virtual environment they are placed in that   includes objects and other actors.



\subsection{Positionnement du projet / Position of the project}
\begin{xcomment}  
Pr\'eciser :
positionnement du projet par rapport au contexte d\'evelopp\'e pr\'ec\'edemment : vis- √†-vis des projets et recherches concurrents, compl\'ementaires ou ant\'erieurs, des brevets et standards'
indiquer si le projet s'inscrit dans la continuit\'e de projet(s) ant\'erieurs d\'ej√† financ\'es par l'ANR. Dans ce cas, pr\'esenter bri\`evement les r\'esultats acquis,
positionnement du projet par rapport aux axes th\'ematiques de l'appel √† projets,
positionnement du projet aux niveaux europ\'een et international.
\end{xcomment}


Animating virtual agents with expressivity is a grand challenge for the entertainment industries (video games, movie industry) that rely mainly on motion capture data which allows them to produce rich and subtle motion but with a high cost in time and finance. On the other hand, technology for interactive agents uses mainly procedural approach. While such approach allows modulating in real-time agents' motion and its quality, the results are still far from being natural and realistic. Lately statistical approaches have been developed. They are promising as they produce animations captur-ing naturalness and richness of human motion. However the control of such animation technique is still an issue and its extension to a large range of motion activities is also an important challenge.

DADA aims to bridge the gap between those previous techniques by proposing a general framework for combining them into a unified interface. A desirable outcome of the project will be a completely novel interaction model for rehearsing with virtual actors and incrementally building complex multi-actor performances with multiple layers of 3D animation. Thus DADA fits the component Information and Communication Society; it is also in line with at least two axes of the ANR call.

\begin{itemize}
\item[First axis:] Le num\'erique au service des arts, du patrimoine, des industries culturelles et \'editoriales (3.7.1.3). There are several potential applications of DADA on top of the proposed virtual theater. The creation of expressive virtual characters can be used in video games, especially for NPC (non-player characters), and in serious games. Indeed being able to simulate the motion of a virtual actor with different expressivities and for different morphologies while maintaining a high level of naturalness and lifelikeness will be a big benefit in time and money.

\item[Second axis: ] Interactions des mondes physiques, de l'humain et du monde num\'erique (3.7.2.4). The outcome of DADA
will benefit the creation of virtual agents, either autonomous or controlled by humans. These agents ought to display a
large variety of communicative and emotional expressions toward human interactants as well as to perform many actions with objects in their virtual environment. Enhancing, in quantity and expressivity, the behaviors of virtual agents is one of the challenges of DADA that falls under the axis.
\end{itemize}

\paragraph{Related projects:} There exist several large European projects that are related to DADA research themes. However, to our knowledge, none covers our research question of building expressive animation with different levels of control. We can name the NoE IRIS on story-telling, the IP Companions on dialog virtual agents, the IP REVERIE on modeling virtual characters in highly immersive virtual environment, and the STREP Ilhaire aims to simulate laughing agent using data-driven and motion graph approaches. On the National side, we can name the Feder project Anipev in which the database Emilya has been captured.

Several recent projects have been devoted to the interface between computers and theatre, e.g. ANR VIRAGE (a generic architecture for controlling lighting and music during theatre production), ANR OSSIA (authoring tools for writing interactive, multimedia scenarios) and ANR INEDIT (INteractivit\'e dans l'Ecriture De l'Interaction et du Temps). ANR Spectacle-en-ligne(s) was a SHS CORPUS project dedicated to capturing, indexing and annotating 200 hours of theatre rehearsals recorded in high-definition video. Those projects focus on the interaction of computer systems with real actors. In contrast, DADA will focus on the core issue of directing virtual actors.

Despite considerable academic research, few procedural animation systems have become commercially available in recent years. Euphoria by Natural Motion is a real-time procedural animation engine, which has been used in Grand Theft Auto 4 and other games. However, actions and expressions are difficult to control. Xtranormal Technologies was an online service for quickly creating 3-D animations from dialogues decorated with stage directions, using a proprietary procedural animation engine limited to non-expressive behaviors.  Actor Machines is a company created by Ken Perlin to commercialize packages of trained virtual actors  with a large range of actions and expressions, which has not delivered any product yet.



\subsection{Etat de l'art / State of the art}
\begin{xcomment}  
Pr\'esenter un \'etat de l'art national et international, en dressant l'\'etat des connaissances sur le sujet. 
Faire appara√Ætre d'\'eventuelles contributions des membres de la proposition de projet √† cet \'etat de l'art.
Faire appara√Ætre d'\'eventuels r\'esultats pr\'eliminaires. 
Inclure les r\'ef\'erences bibliographiques n\'ecessaires au ß 7.
\end{xcomment}

A large body of theoretical research work relates to acting and directing in the theatre, especially from a cognitive science
perspective \cite{Kemp2010}. Its applicability to virtual actors is limited because of the huge gap between virtual and human acting skills.
An extensive survey of acting techniques used in 3D animation and virtual worlds can be found in the excellent collection edited 
by Tanenbaum, et al. \cite{Tanenbaum2014}. The priomises and limitations of virtual actors  in contemporary 
theatre has been surveyed by Dixon \cite{Dixon2007}, Salter \cite{Salter2010} and Bourassa and Poissant \cite{Bourassa2013}.


\subsubsection{Single-character animation}

Animation of an avatar is usually tackled by working separately on the full body animation model on the one hand and on the face (and gesture) animation model on the other hand (since the latter animation strongly depends on the dialogue the avatar is engaged in), where the animation produced by the two models are merged to produce a final complete animation \cite{DBLP:journals/tvcg/ShoulsonMKB14}. 

Full body kinematic animation (or control) consists in animating the full body of an avatar while he is performing actions such as walking, dancing, sitting etc. Although there has been lots of work on this subject it is still a challenging problem due to the high dimensionality of the character's configuration. Data-driven approaches are very popular here and make of use motion-capture data to learn  animation models which, once learned, may be used to animate a virtual character to perform a given task. Many animation techniques based on concatenation of motion capture data and clips selection~\cite{Lee2002,Kovar:2002:MG,Heck:2007:PMG} have been proposed. These methods focus on how to improve the clips searching performance or the transition quality between successive clips. These methods have been applied very successfully to locomotion synthesis. Feng et al.~\cite{Feng:2012:EMS} have extended motion example method for gesture synthesis. Since they use directly motion capture, their synthesized result is quite natural, but do suffer from some limitations: new data needs to be recorded for any new required animation.


Many systems have been proposed for producing animation models and controlers using  statistical models such as Hidden Markov Models (HMMs) \cite{DBLP:journals/tog/LevineTK09} and Conditional Random Fields (CRFs) \cite{DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14}. Most accurate methods exploit a large dataset of motions where one can synthesize a complete motion sequence corresponding to a particular task by using warping or blending strategies of motions in the training set \cite{DBLP:conf/siggraph/WitkinP95}. Locomotion controllers have been proposed that concatenate motion clips from a motion capture dataset to produce an animation that is smooth \cite{DBLP:journals/tog/TreuilleLP07, DBLP:journals/tog/McCannP07}. High-quality kinematic controllers have been built from this idea by using a {\it motion graph}, which is a graph structure that describe how clips from a dataset can be reordered into new motions \cite{Lee2002}. 


Style Machines~\cite{Brand:2000:SM} achieve the stylistic motion synthesis by learning motion patterns from a highly varied set of motions, as a distinct choreography can perform motions with a distinctive style. Stylistic Hidden Markov models (SHMM) are proposed to train different stylized behaviors. A new style motion can also be obtained from the interpolation of SHMM sub-spaces. This approach treats animations as a pure data modeling task. Later, such types of work have been extended~\cite{Wang:2003:LKH}~\cite{Pan:2009:UHM}, but, so far, the focus of these  synthesis works is more on cyclic motion; cyclic motions are somehow easier to treat for clustering, blending or filling missing data. 

While locomotion controllers are driven by direct high-level commands (such as desired movement direction), no such clear control signal is available for body language. To animate the face, and accompanying arm gestures, many works have focused on developing specific animation models based on a dialogue related input, either speech, text or prosody features \cite{DBLP:journals/tog/LevineTK09, DBLP:journals/tog/LevineKTK10, DBLP:conf/atal/ChiuM14, ding2014upper}.  Finally, recent work has demonstrated such models for the case of locomotion believable controllers, gesture controllers \cite{DBLP:journals/tog/LevineKTK10} and face controllers \cite{DBLP:conf/icassp/DingRAP13}. 

% We will aim to generalize this previous work
% to more general action controllers, including such actions as: sitting, standing, walking, grasping, taking and putting
% objects, in a variety of expressions and moods.
Yet all these statistical approaches require large annotated datasets to work well. Thereby these approaches do not easily work with small training sets which is a key issue, as stressed for instance in \cite{DBLP:journals/tog/LevineWHPK12}, since first it requires considerable effort and time to build large datasets, and second because many applications demand unique motion styles and require their own datasets. This has led a number of researchers to put the effort on designing models that may be easily learned from a few samples. One main approach for doing so lies in the use (or learning) of a continuous state space to represent the data, making learning in this low dimensional space much easier  \cite{DBLP:journals/tog/LevineWHPK12, DBLP:conf/atal/ChiuM14}. A relevant technology for this are Gaussian Process which have been extended for dealing wiuth dynamic data in \cite{DBLP:journals/pami/WangFH08}.

These latter models are not far from recurrent neural networks, and to Long Short Term Memory neural networks in particular \cite{DBLP:conf/nips/HochreiterS96, DBLP:journals/corr/GreffSKSS15}, that have been shown recently to work well for complex signals such as speech and handwriting, for recognition tasks \cite{DBLP:conf/nips/GravesS08} as well as for synthesis tasks \cite{DBLP:journals/corr/Graves13}. These models are part of a current trend in machine  learning called representation learning (see the recently born conference ICLR at http://www.iclr.cc/) which aims at discovering relevant and usually low dimensional representation of the data under investigation (the pionner work of this domaine is the one by G. Hinton in Science \cite{DBLP:journals/cogsci/HintonOWT06}.

\subsubsection{Multiple-character animation}

\paragraph{ECAs gathering in groups}: Prada and Paiva \cite{Prada2005} modeled groups of autonomous synthetic virtual agents that collaborated with the user in the resolution of collaborative tasks within a 3D virtual environment. Rehm and Endrass \cite{RehmEndrass09}  implemented a toolbox for modeling the behavior of multi-agent systems. In \cite{RavenetCOP14}, Ravenet and colleagues combined a number of reactive social behaviors, including those reflecting personal space \cite{Hall69} and the F-formation system \cite{Kendon90}, in a general steering framework inspired by \cite{Reynolds99}. This complete management of position and orientation is the foundation of the Anonymous Engine used in the work presented here. All these models did not take into account the expression of attitudes while exhibiting the behaviors of the agents.

Forward Kinematics (FK) and Inverse Kinematics (IK) have been used to generate animation sequences procedurally~\cite{Boulic:2006:EOA,Unzueta:2008:FPA}.
Different constraints and unconstrained numerical equation solvers are used to model the motion postures and their transitions~\cite{Zhao:1994:IKP}. The target based reaching models, such as Jacobian methods~\cite{Yuan:2001:LSI}~\cite{Buss:2004:SDL}, are proposed to use linear approximation to bring the end-effector close to the target; it can be used to build a posture configuration for virtual characters. These methods are flexible and widely used for real time lost cost procedural generation and motion editing, but the resulting motions may not be as realistic as human motion. They can show quite some stiffness. 

There exist several systems embedded in large platform that combine different animation synthesis solutions. The Smartbody system~\cite{Shapiro:BCA}~\cite{Thiebaux:2008:SBR} includes a schedule controller and a realization controller. Smartbody includes different motion synthesis methods such as motion capture data and dynamic system.
The behavior engine developed by Marsella et al.~\cite{Marsella:2013:VCP} makes use of Smartbody. This engine embeds a rule-based system that can determine facial expressions and gestures by extracting semantic, pragmatic and rhetorical content from an input utterance. Some behaviors (eg head motion) are also obtained through data-driven model. Luo et al.~\cite{Luo:2009:AGA} proposed a procedural arm gesture model. They improve the quality of the procedural animation by introducing motion capture data for full body animation. ADAPT~\cite{DBLP:journals/tvcg/ShoulsonMKB14} is also a flexible platform for virtual human characters. This system has been designed as a gaming system for physical reactions.  They use a behavior tree to model human-like behaviors for multi-characters and a Level of Detail character shadows solution is proposed to generate body parts sequentially. Blending technique is applied to compute the movements of different body parts from different Choreographers. While some motion artifacts may appear when doing blending or character motion transition, this animation platform combines various animation techniques. However it has not been focused on communicative and emotional behaviors that are defined by specific temporal patterns.


Existing animation techniques, procedural and data-driven, have pros and cons. With the procedural animation, gestures are described using the symbolic language BML \cite{Kopp2006}. New behaviors can be created very rapidly; but, most of the time, the obtained animation lacks naturalness and fluidity. On the other hand animation from motion-capture data reproduces all human motion subtleties and dynamics; however creating new gesture is more cumbersome as it requires new recording of data and new training. Our aim in DADA will be to develop an embodied conversational agent system that embeds both animation approaches. Communicative behaviors are computed procedurally while socio-emotional behaviors such as emotions are driven from machine-learning techniques (Task 1.2). Both computations involve the same body parts. While the two animation streams are computed separately they need to be merged to produce the final animation output (Task 2.3).  

\subsubsection{Authoring tools and metaphors} There has been previous work on story-driven architectures and directing tools specifically dedicated
for virtual theatre. The desktop theatre by Strassmann \cite{Strassmann1992},  the Improv  system by Perlin and Goldberg \cite{Perlin1996} and the story-driven architecture by Pinhanez \cite{Pinhanez2000} are seminal works in building virtual actors that can be scripted to perform on a virtual stage.  Other early work in Europe includes the Pinnochio \cite{Maiocchi90} and GEIST \cite{Spierling02} projects, which targetted players of interactive games by letting them "play director". The acting capabilities of those early systems are limited and the amount of programming 
needed to produce even the simplest scene is substantial.

Xtranormal Technologies Text-To-Movie was an online application for quickly creating 3D animation by writing dialogues with emoticons, which were automatically
translated into animation using a combination of procedural and data-driven animation. The DRAMA project at the University of Toulouse has also investigated this paradigm.
While automatic text-to-scene animation is a valid research area, it has the disadvantage that it is taking control away from the director. In DADA, we take
a different route, which is offer maximum control to the director over the virtual actors.

MIRAGE \cite{El-Nasr04} is an interactive story generation engine featuring 3D animation of two virtual actors playing the roles of Electra and Archemedis in a tragic style. Actions 
are represented by a verb, and adverb and an actor; and are either controled by the player or generated by the system. Actor's behaviours are organized into dramatic beats with multiple
actions and reactions based on communicative goals. While the focus of MIRAGE is different from DADA, it offers the important insight that actions performed by the virtual actors must be appraised from the point of view of the user (director, audience or player).  This will be used  as a general guideline in DADA as well.  

%The body action and posture coding system (BAP)  \cite{Dael12a} is an extensive description language for body movement on the anatomical level (body parts), the form level (directions and orientations of movements) and the function level (communicative and goal-driven actions). Their work is important for DADA because it offers a catalogue of  expressive gestures used by professional actors in depicting various emotions \cite{Dael12b}. The language makes it possible to precisely annotate body actions in video using the ANVIL annotation tool \cite{Kipp01,KIPP12}.

The Q language \cite{Ishida2002} is an authoring language for writing scenarios involving multiple autonomous agents. While not targeting theatre
as an application, the language borrows heavily from theatre practices and is based on defining  cues that synchronize  the actions and reactions 
of the agents. Q is a dialect of the scheme language and assumes that the scenario writer is familiar with programming. The directing language
for DADA will also be based on cues and actions, but will be extended to include direct user manipulation using a graphical user interface, rather
than a programming interface. Furthermore, animation generated with the Q language was fairly primitive and we plan to offer a more expressive
and extensive description of the actor's full body animation. 

The movie script markup language (MSML) \cite{VanRijsselbergen2009} has been proposed to encode the stucture of movie  and play scripts into scenes, actions and dialogues. One interesting feature of the language is that it includes an animation layer, making it possible to compute a realization of the play script in 2D animation. The synchronization of the various actors and actions
in the play script is performed by generating an Object Composition Petri Net (OCPN) \cite{Little06} for the entire scene. This feature has not been demonstrated with 3D animation, and it will be one of the objective of the DADA project to implement, test and validate the MSML animation layer with multiple autonomous 3D actors in a real-time game engine. 

Petri nets have also been proposed as a high-level specification for virtual actor behaviours \cite{Magalhaes98,Blackwell01},
including the  important case of turn-taking in dialogue and imitation games \cite{Chao11a,Chao11b}. They are also the basis for the interactive 
musical score system developed by the INEDIT project \cite{Allombert08,Marczak11}, which is being extended to multimedia events \cite{Toro12}. 
Petri nets are likely  candidates to become  an internal, intermediate representation  of the performance score in DADA,  between the high level 
commands of the director and the low-level executable finite-state machine of the game engine.   In previous work,  the composer or director 
manipulates the Petri nets directly. In DADA, we will instead offer direct manipulation of cue sheet  and prompt books, which offer  more natural 
interaction for theatre directors than Petri net places and transitions. 

\subsubsection{Robotic actors}

Our project is also related to the emerging field of  robotic actors performing theatre on a live stage  \cite{Lin2013}. An international workshop was dedicated to
robotics and theatre at ICRA 2012 \footnote{Robotics and Performing Arts: Reciprocal Influences, http://www.robotics-and-performing-arts.sssup.it/}. Our 
research on directing autonomous digital actors will likely be applicable to the case of robotic actors as well. 


\subsubsection{Prior art at LITC}
Our developments will be based on the GRETA platform and use the EMILYA database. The Greta platform simulates virtual agents able to communicate verbally and nonverbally with human users and/or other virtual agents. Given a set of intentions and emotions to be communicated, the platform instantiates them into sequences of synchronized nonverbal behaviours. It can be used to compute these multimodal behaviours when the virtual agent acts as a speaker or as a listener.

The Greta system allows a virtual or physical (e.g. robotic) embodied conversational agent to communicate with a human user \cite{Ochs2013emoti,Niewiadomski2011}. It is a SAIBA compliant architecture (SAIBA is a common framework for the autonomous generation of multimodal communicative behavior in Embodied conversational agents \cite{Kopp2006}). The main three components are: (1) an \textit{Intent Planner} that produces the communicative intentions and handles the emotional state of the agent; (2) a \textit{Behavior Planner} that transforms the communicative intents received in input into multimodal signals and (3) a \textit{Behavior Realizer} that produces the movements and rotations for the joints of the ECA.

%A \textit{Behavior Lexicon}  contains pairs of mappings from communicative intentions to multimodal signals. The Behavior Realizer instantiates the multimodal behaviors, it handles the synchronization with speech and generates the animations for the ECA. 

%The information exchanged by these components is encoded in specific representation languages defined by SAIBA. The representation of communicative intents is done with the Function Markup Language (FML) \cite{FML2008IVA}. FML describes communicative and expressive functions without any reference to physical behavior, representing in essence what the agent's mind decides. It is meant to provide a semantic description that accounts for the aspects that are relevant and influential in the planning of verbal and nonverbal behavior. Greta uses an FML specification named \textit{FML-APML} and based on the Affective Presentation Markup Language (APML) introduced by \cite{APMLDeCarolis2004}. FML-APML tags encode the communicative intentions following the taxonomy defined by \cite{Poggi2007}, where a communicative function corresponds to a pair (\textit{meaning},\textit{signal}). The meaning element is the communicative intent that the ECA aims to accomplish, whereas the signal element indicates the multimodal  behavior exhibited in order to achieve the desired communicative intent. The multimodal behaviors to express a given communicative function to achieve (e.g. facial expressions, gestures and postures) are described by the Behavior Markup Language (BML) \cite{Kopp2006}.

%The information exchanged by these components is encoded in specific representation languages defined by SAIBA. The representation of communicative intents is done with the Function Markup Language (FML) \cite{FML2008IVA}. FML describes communicative and expressive functions without any reference to physical behavior, representing in essence what the agent's mind decides. It is meant to provide a semantic description that accounts for the aspects that are relevant and influential in the planning of verbal and nonverbal behavior. Greta uses an FML specification named \textit{FML-APML} and based on the Affective Presentation Markup Language (APML) introduced by \cite{APMLDeCarolis2004}. FML-APML tags encode the communicative intentions following the taxonomy defined by \cite{Poggi2007}, where a communicative function corresponds to a pair (\textit{meaning},\textit{signal}). The meaning element is the communicative intent that the ECA aims to accomplish, whereas the signal element indicates the multimodal behavior exhibited in order to achieve the desired communicative intent. The multimodal behaviors to express a given communicative function to achieve (e.g. facial expressions, gestures and postures) are described by the Behavior Markup Language (BML) \cite{Kopp2006}.

%Lately we have been developing data-driven approach to capture the link between acoustic features (speech \cite{Ding2013}, laughter \cite{ding2014upper,Ding2014}) and multimodal behaviors. The obtained animations show more subtle motions than those generated with the procedural model. 

Data-driven animation in DADA will use the Emilya database (Emotional body expression in daily actions database) \cite{FouratiEmilya2014}.  Eleven (unprofessional) actors participated in the data collection. Both 3D motion capture data (using Xsens technology \cite{XsensSystem}) and audio visual data were recorded and synchronized. The actors were asked to express 8 emotions (Joy, Anger, Panic Fear, Anxiety, Sadness, Shame, Pride and Neutral) in 7 daily actions (walking, waking with an object in the hands, sitting down, knocking at a door, lifting and throwing an object (a ball made of paper) with one hand, and moving objects (books) on a table with two hands) \cite{FouratiEmilya2014}. Those emotions were selected to cover the arousal and valence dimensions. We asked the actors to perform each action four times in a row to capture a large set of data. A continuous sequence consisting of the series of all the actions with just one trial per action was also recorded.  After segmentation, we obtain a database of around 10000 segments depicting expressive body movements. Moreover we have validated this database through perceptual studies. 


\subsubsection{Prior art at Inria}
Previous work in the IMAGINE team can be useful to the DADA project, including work on sketch-based modeling of actor's movements  using motion brushes \cite{milliez:hal-01056600} and lines of action \cite{Guay2015};  implicit skinning  of actor's body shapes for improved rendering of character animation \cite{vaillant:hal-00819270}; and audio-visual prosody modeling for expressive facial animations of actors \cite{Barbulescu2014}. All this  previous work will be made available to the DADA project.


\subsection{Objectifs et caract\`ere ambitieux/novateur du projet / Objectives, originality and novelty of the project}
\begin{xcomment}  
D\'ecrire les objectifs du projet et d\'etailler les verrous scientifiques et techniques √† lever par la r\'ealisation du projet. Insister sur le caract\`ere ambitieux et/ou novateur de la proposition.
D\'ecrire \'eventuellement le ou les produits finaux d\'evelopp\'es, pr\'esenter les r\'esultats escompt\'es en proposant si possible des crit\`eres de r\'eussite et d'\'evaluation adapt\'es au type de projet, permettant d'\'evaluer les r\'esultats en fin de projet.
\end{xcomment}  

The fundamental research program proposed in the DADA project  targets the "grand challenge" of getting virtual actors to perform  a theatre scene together. This has never been demonstrated before. DADA is therefore a high-risk, high-gain project, with the ambition to significantly raise the level of believability and expressivity of autonomous conversational agents by endowing them with acting skills.  Working with theatre directors and scholars will raise the bar of what is expected of virtual actors created in DADA both in terms of aesthetic quality and authoring tool usability. By leveraging existing animation techniques developed previously by the DADA partners in their respective fields, the DADA project will be starting from a solid position, but this  will not be sufficient 
to meet the theatre world's expectations. 

The expected results of DADA will be (1) a virtual theatre company of autonomous actors with a large vocabulary of expressive animation skills; and (2) a prototype system for directing arbitrary dramatic plays, amenable to a variety of digital storytelling applications. Results will be integrated into Unity3D which is already used by the GRETA plat-form at Telecom ParisTech and the virtual cinematography framework developed by the IMAGINE team at Inria. Results will be used at University of Marseille for building a pivot actor model allowing the retargeting of the DADA actors to actors with different morphologies and styles. Results will be used by Paris 8 as a virtual rehearsal space for theatre productions involving real actors interacting with digital actors, and as a platform for publishing digital dramatic performances online. If applicable, results will also be patented and exploited by the three academic partners, targeting commercial applications such as video games, digital storytelling, virtual  worlds and movie previz.


%\subsubsection{Virtual theatre company}
%
%\paragraph{Proxemic models} The role of the proxemic models is to compute the precise positions and orientations of actors at all time, given the director's blockings.
%
%\paragraph{Kinesic models} The role of the kinesic models is to compute the remaining degrees of freedom, given the director's blockings and the precise positions and orientations of actors at all time. 
%
%One difficulty will be to generate those remaining degrees of freedom with high quality, avoiding the robotic effects associated with procedural animation,
%and the repetitive effects associated with data-driven methods. 
%
%More precisely, we  will work to make each performance plausible (actor maintains personality of the role), expressive (actor follows director's commands)
%and natural (actor adapts to the environment with variations)
%
%\subsubsection{Directing tools}
%
%Expose all important dramatic parameters to the director ; compute all other parameters at runtime.
%
%\paragraph{Real-time animation and synchronization}
%
%Our solution is based on a prompter system.
%
%T

To confront this grand challenge, the first main bottleneck is the very large dimension of the parameter space of character animation (50-80 degrees of freedom per actor).  We will therefore  decompose the parameter space into a hierarchy of nested subspaces: (a)  blocking parameters controled directly by the director, including actions, attitudes, stage positions and trajectories, etc. ; (b) proxemic  parameters computed by the autonomous actors in relation to each other, to the stage and to the audience,  given the blocking directions; (c) kinesic  parameters  computed by the the autonomous actors to realize the blocking directions, given their proxemic relations. The second main bottleneck is the lack of functional-level description language for non-verbal communication,
and we will adapt working practises from the theatre world to propose such a description language in the form of a performance score notation . 



\endinput
